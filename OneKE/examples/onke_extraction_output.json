{
  "result": "Here's a summary of the information extracted from the result list:\n\n**Company Information**\n\n* Company Name: University of Chicago, Booth School of Business\n* Address: 5757 S. University Ave. Chicago, IL 60637\n* Phone: 773.702.5599\n* Website: bfi.uchicago.edu\n\n**Executive Team**\n\n* Executive Members:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**Financial Metrics**\n\n* Financial Metrics:\n  + Earnings\n  + Revenue\n  + Net Income\n  + Cash Flow\n\n**Research Design**\n\n* Research Design:\n  + Passing a balance sheet and income statement in a standardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze them.\n  + Based on the analysis of the two financial statements, the model must decide whether a firm's economic performance is sustainable and, more specifically, whether a company's earnings will grow or decline in the following period.\n\n**Research Questions**\n\n* Research Questions:\n  + Can an LLM successfully perform financial statement analysis in a way similar to what professional human analysts do?\n\n**Authors**\n\n* Authors:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**LLM Performance**\n\n* LLM Performance:\n  + GPT's performance is at a disadvantage compared to human analysts due to the complexity and ambiguity of financial statement analysis.\n  + LLM's performance is on par with the performance of a narrowly trained state-of-the-art ML model.\n  + GPT's performance comes from its memory, e.g., due to the model's ability to identify the company based on numeric data.\n  + The strength of the model is in its ability to generate narrative insights based on its analysis of numeric data.\n\n**Model Comparison**\n\n* Model Comparison:\n  + GPT vs. ANN: GPT outperforms ANN in some cases, but ANN performs better in others.\n  + GPT vs. Logistic Regression: GPT outperforms Logistic Regression in some cases, but Logistic Regression performs better in others.\n\n**Output Schema**\n\n* Output Schema:\n  + Properties:\n    - Company\n    - Executive\n    - Merger\n    - Transaction\n    - Reports\n    - Discloses\n    - Increases\n    - Decreases\n    - Forecasts\n    - Impacts\n    - Financial Metric\n    - Fiscal Period\n    -",
  "trajectory": {
    "instruction": "",
    "text": "5757 S. University Ave. \nChicago, IL 60637 \nMain: 773.702.5599 \nbfi.uchicago.eduWORKING PAPER ·  NO. 2024-65\nFinancial Sta tement Analysis with Large \nLanguage Models\nAlex G. Kim, Maximilian Muhn , and Valer i V. Nikolaev\nMAY 2024\n\nFinancial Statement Analysis with\nLarge Language Models\nAlex G. Kim1Maximilian Muhn2Valeri V. Nikolaev3\nThis draft: May 20, 2024\nAbstract\nWe investigate whether an LLM can successfully perform financial statement analy-\nsis in a way similar to a professional human analyst. We provide standardized and\nanonymous financial statements to GPT4 and instruct the model to analyze them to\ndetermine the direction of future earnings. Even without any narrative or industry-\nspecific information, the LLM outperforms financial analysts in its ability to predict\nearnings changes. The LLM exhibits a relative advantage over human analysts in sit-\nuations when the analysts tend to struggle. Furthermore, we find that the prediction\naccuracy of the LLM is on par with the performance of a narrowly trained state-of-\nthe-art ML model. LLM prediction does not stem from its training memory. Instead,\nwe find that the LLM generates useful narrative insights about a company’s future\nperformance. Lastly, our trading strategies based on GPT’s predictions yield a higher\nSharpe ratio and alphas than strategies based on other models. Taken together, our\nresults suggest that LLMs may take a central role in decision-making.\nKeywords: Financial statement analysis, Large language models, GPT4, chain-of-\nthought, neural network, asset pricing, earnings, direction of earnings changes, analysts\nJEL Codes: G12, G14, G41, M41\nCompanion App: To showcase the capabilities of LLMs for financial statement anal-\nysis, we created an interactive Companion App .*\n1The University of Chicago, Booth School of Business, alex.kim@chicagobooth.edu\n2The University of Chicago, Booth School of Business, maximilian.muhn@chicagobooth.edu\n3The University of Chicago, Booth School of Business, valeri.nikolaev@chicagobooth.edu\nWe appreciate insightful comments from Bok Baik, Mark Bradshaw, Joachim Gassen, Ralph Koijen,\nLaurence van Lent, Christian Leuz, and Sanjog Misra, and workshop participants at the Bernstein Quanti-\ntative Finance Conference, University of Chicago, University of North Carolina at Chapel Hill, 2024 Tuck\nAccounting Spring Camp at Dartmouth, and Korean-American Accounting Professors’ Association. Yijing\nZhang provided excellent research assistance. The authors gratefully acknowledge financial support from the\nUniversity of Chicago Research Support Center, Fama-Miller Center for Finance Research, and the Stevens\nDoctoral Program at the University of Chicago Booth School of Business.\n*This app requires ChatGPT Plus subscription and relies on a different prompt that integrates narrative\ncontext while processing 10-Ks and 10-Qs step-by-step. The downside of this functionality is that it is more\nprone to retrieval errors and the accuracy of information must be verified.\nFinancial Statement Analysis with Large Language Models 1\n1 Introduction\nCan large language models (LLMs) make informed financial decisions or are they simply\na support tool? Their advanced capabilities to analyze, interpret, and generate text enable\nLLMs to excel across a wide range of tasks, including summarization of complex disclosures,\nsentiment analysis, information extraction, report generation, compliance verification, etc.\n(e.g., Bernard et al., 2023; Bybee, 2023; Choi and Kim, 2023; Kim et al., 2023a,b; Lopez-Lira\nand Tang, 2023). All these tasks, however, involve the textual domain and require specialized\ntraining or fine-tuning of the model.1The boundaries of this disruptive technology outside\nof the textual domain and with respect to more general tasks that require numeric analysis\nand judgment are yet to be understood. We probe these boundaries in the financial analysis\ndomain.\nWe study whether an LLM can successfully perform financial statement analysis in a\nway similar to what professional human analysts do. The answer to this question has far-\nreaching implications for the future of financial analysis and whether financial analysts will\ncontinue to be the backbone of informed decision-making in financial markets. The answer\nis far from obvious, given that an LLM lacks the deep understanding of the financials of a\ncompany that a human expert would have. Further, one of the most challenging domains for a\nlanguage model is the numerical domain, where the model needs to carry out computations,\nperform human-like interpretations, and make complex judgments (Brown et al., 2020).\nWhile LLMs are effective at textual tasks, their understanding of numbers typically comes\nfrom the narrative context and they lack deep numerical reasoning or the flexibility of a\nhuman mind.\nFinancial statement analysis (FSA), sometimes referred to as fundamental analysis, is\na particularly useful setting to examine the role of LLMs in future decision-making. Tra-\nditionally, financial statement analysis is performed by financial analysts and investment\nprofessionals with the primary objective to understand the financial health of a company\nand determine whether its performance is sustainable. Unlike a typical task performed by\nan LLM, FSA is a quantitative task that involves analyzing trends and ratios. At the same\ntime, it also requires critical thinking, reasoning, and ultimately, complex judgments. Im-\nportantly, unlike in other applications, such as answering bar or CPA exam questions (Choi\net al., 2022; Eulerich et al., 2023), an LLM cannot rely on its memory for the correct answer.\nOur research design involves passing a balance sheet and income statement in a stan-\ndardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze\n1For example, to be able to efficiently summarize texts, an LLM is trained on a large corpus of documents\nthat involve summaries typically generated by humans\nFinancial Statement Analysis with Large Language Models 2\nthem. In particular, based on the analysis of the two financial statements, the model must\ndecide whether a firm’s economic performance is sustainable and, more specifically, whether\na company’s earnings will grow or decline in the following period. We focus on earnings\nbecause they are the primary variable forecasted by financial analysts and fundamental for\nvaluation (Penman and Sougiannis, 1998; Penman, 2001; Monahan et al., 2018).\nA key research design choice that we make is to notprovide any textual information\n(e.g., Management Discussion and Analysis) that typically accompanies financial statements.\nWhile textual information is easy to integrate, our primary interest lies in understanding the\nLLMs’ ability to analyze and synthesize purely financial numbers. We use this setup to\nexamine several research questions.\nFirst, can a large language model generate economic insights purely from the numbers\nreported in financial statements absent any narrative context? How does an LLM’s per-\nformance compare to that of human analysts and do they add incremental value? Can\nthe model’s performance be enhanced via instructions that emulate steps typically followed\nby financial analysts? How does LLM’s performance compare to other benchmarks, such\nas logistic regression and a state-of-the-art ANN design, and whether the model can offer\nadditional insights?\nConceptually, an LLM can add value relative to a human analyst due to its ability\nto quickly analyze large quantities of unstructured data and a vast knowledge base that\nenables to model to recognize patterns, e.g., familiar business situations, in the data. It is\nnot obvious, however, that these considerations are particularly relevant for the present task.\nIn fact, there are a number of reasons to expect that professional analysts will outperform a\nmachine-based approach to financial statement analysis. First, financial statement analysis\nis a complex and loosely defined task that involves ambiguity and requires common sense,\nintuition, and flexibility of the human mind. Second, it requires reasoning and judgment that\nmachines presently lack. Finally, it necessitates a broader understanding of the industry and\nmacro-economy.\nWhen compared to a narrowly specialized ML application, such as an artificial neural\nnet (ANN) trained for earnings prediction, an LLM also appears to be at a serious disadvan-\ntage. Training a specialized ANN allows the model to learn deep interactions that contain\nimportant cues that cannot be easily gathered by the general-purpose model without pro-\nviding additional insights or context. Nevertheless, an LLM’s advantage potentially lies in\nits vast knowledge and general understanding of the world, such as business concepts and\ninvestment theories that allow it to emulate deductive reasoning performed by humans. This\ncould include intuitive reasoning and forming hypotheses based on incomplete information\nor previously unseen scenarios.\nFinancial Statement Analysis with Large Language Models 3\nOur approach to testing an LLM’s performance involves two steps. First, we anonymize\nand standardize corporate financial statements to prevent the potential memory of the com-\npany by the language model. In particular, we omit company names from the balance sheet\nand income statement and replace years with labels, such as t, andt−1. Further, we\nstandardize the format of the balance sheet and income statement in a way that follows\nCompustat’s balancing model. This approach ensures that the format of financial state-\nments is identical across all firm-years so that the model does not know what company or\neven time period its analysis corresponds to.\nIn the second stage, we design prompts that instruct the model to perform financial\nstatement analysis and, subsequently, to determine the direction of future earnings.2In\naddition to a simple prompt, we develop a Chain-of-Thought (CoT) prompt that effectively\n“teaches” the model to mimic a financial analyst.3In particular, as a part of their analysis,\nfinancial analysts identify notable trends in financial statement line items, compute key\nfinancial ratios (e.g., operating efficiency, liquidity, and (or) leverage ratio), synthesize this\ninformation, and form expectations about future earnings (Bouwman et al., 1987). Our\nCoT prompt implements this thought process via a set of instructions ultimately making\na determination of whether next year’s earnings will increase or decrease compared to the\ncurrent year.\nWe test the model’s performance using the Compustat universe and, when necessary,\nintersect it with the IBES universe. The full sample spans the 1968-2021 period and includes\n150,678 firm-year observations from 15,401 distinct firms. The analyst sample spans the\n1983-2021 period with 39,533 observations from 3,152 distinct firms. Our target variable\nacross all models is a directional change in future earnings. To evaluate analysts’ prediction\naccuracy, we compute consensus forecasts (the median of individual analyst forecasts issued\nin the month following the release of financial statements) and use them as an expectation for\nthe following year’s earnings. This ensures the comparability of analysts’ forecasts and model\nprediction results.4In addition, we also use three-month and six-month ahead consensus\nforecasts as alternative expectation benchmarks. These benchmarks disadvantage the LLM\nas they incorporate the information acquired during the year. However, because analysts\n2Focusing on predicting the direction of future earnings provides a specific and measurable objective,\nfacilitating the benchmarking of the model’s performance. It is also consistent with early and more recent\nliterature on this topic (e.g., Ou and Penman, 1989; Chen et al., 2022). Additionally, the focus on a binary\nvariable is also motivated by the notion that most key decisions performed by humans are binary in nature\n(e.g., Kahneman, 2011).\n3Chain-of-thought prompts are known to enhance the model’s problem-solving capability and induce\nhuman-like reasoning (Wei et al., 2022).\n4Since the quantitative models use only financial statement variables, we thus align the timing of human\nforecasts with the timing of AI-based forecasts.\nFinancial Statement Analysis with Large Language Models 4\nmay be sluggish to incorporate new information into their forecasts, we report them for\ncomparison purposes.\nWe start by analyzing GPT’s performance compared to security analysts in predicting the\ndirection of future earnings (Ou and Penman, 1989). At the outset, we note that predicting\nchanges in EPS is a highly complex task as the EPS time series are approximated by a random\nwalk and contain a large unpredictable component. We find that the first-month analysts’\nforecasts achieve an accuracy of 53% in predicting the direction of future earnings, which\ndominates the 49% accuracy of a naive model that extrapolates the prior year’s change.5\nThree- and six-month ahead forecasts achieve a meaningfully higher accuracy of 56% and\n57% respectively, which is intuitive given that they incorporate more timely information.\nA “simple” non-CoT prompt GPT-based forecasts achieve a performance of 52%, which\nis lower compared to the analyst benchmarks, which is in line with our prior. However, when\nwe use the chain of thought prompt to emulate human reasoning, we find that GPT achieves\nan accuracy of 60%, which is remarkably higher than that achieved by the analysts. Similar\nconclusions follow if we examine the F1-score, which is an alternative metric to evaluate a\nmodel’s forecasting ability (based on a combination of its precision and recall). This implies\nthat GPT comfortably dominates the performance of a median financial analyst in analyzing\nfinancial statements to determine the direction a company is moving in.\nWe probe deeper to understand the strengths and weaknesses of humans relative to an\nLLM. Intuitively, human analysts may rely on soft information or a broader context not\navailable to the model and thus add value (Costello et al., 2020; Liu, 2022). Indeed, we\nfind that analysts’ forecasts contain useful insights about future performance not captured\nby GPT. Furthermore, we show that when humans struggle to come up with the future\nforecast, GPT’s insights are more valuable. Similarly, in the instances where human forecasts\nare prone to biases or inefficiency (i.e., not incorporating information rationally), GPT’s\nforecasts are more useful in predicting the direction of future earnings.\nAs human forecasts are known to exhibit statistical biases (Abarbanell and Bernard, 1992;\nBasu and Markov, 2004), it is also interesting to examine GPT’s performance relative to\nspecialized ML applications trained specifically to predict earnings based on a large dataset.\nWe examine three such forecasting models. The first model follows Ou and Penman (1989)\nand relies on a stepwise logistic regression model with 59 predictors.6Our second model\nis an artificial neural network (ANN) that uses the same 59 predictors but also leverages\n5This finding is consistent with Bradshaw et al. (2012), who show that analysts are superior in predicting\none-year ahead earnings.\n6We exclude predictors that rely on stock prices, in particular the P/E ratio, because balance sheets\nand income statements do not contain stock price information. This exclusion ensures comparability of our\nbenchmark. The results are qualitatively similar, however, if we include this variable.\nFinancial Statement Analysis with Large Language Models 5\nnon-linearities and interactions among them. Third, to ensure consistency between GPT and\nANN, we also use the ANN model trained on the same information set (the income statement\nand balance sheet) that we provide to GPT. Importantly, we train these models each year\nbased on five years of historical data using a population of observations on Compustat. All\nforecasts are out of sample.7\nUsing the entire Compustat sample, we find that the stepwise logistic regression achieves\nan accuracy (F1-score) of 52.94% (57.23%), which is on par with human analysts and consis-\ntent with the prior literature (Ou and Penman, 1989; Hunt et al., 2022). In contrast, ANN\ntrained on the same data achieves a much higher accuracy of 60.45% (F1-score 61.62), which\nis in the range of the state-of-the-art earnings prediction models. When we use GPT CoT\nforecasts, we observe that the model achieves an accuracy of 60.31% on the entire sample,\nwhich is very similar to the ANN’s accuracy. In fact, GPT exhibits a meaningfully higher F1\nscore compared to the ANN (63.45% vs. 61.6%). When we train the ANN exclusively using\nthe data from the two financial statements (fed into GPT), which is a smaller information\nset, we find that ANN’s predictive ability is slightly lower, with an accuracy (F1-score) of\n59.02% (60.66%), compared to GPT’s performance. Overall, these findings suggest that\nGPT’s accuracy is on par (or even slightly higher) than the accuracy of narrowly specialized\nstate-of-the-art machine learning applications. This is a somewhat surprising result because\nspecialized models are trained to leverage information most efficiently. It indicates a remark-\nable aptitude a pre-trained large language model possesses to analyze financial statements\nand even more so given that we do not provide any textual disclosures, such as MD&A.\nWe further observe that ANN’s and GPT’s predictions are complementary in that both\nof them contain useful incremental information with some indication that GPT tends to\ndo well when ANN struggles. In particular, ANN predicts earnings based on the training\nexamples it saw in the past data, and given that many of the examples are complex and\nhighly multidimensional, its learning capacity may be limited. In contrast, GPT makes\nrelatively fewer mistakes when predicting the earnings of small or loss-making companies,\nlikely benefiting from its human-like reasoning and extensive knowledge. This ability to draw\nupon a broader range of knowledge provides a distinct advantage for the language model.\nWe perform several additional experiments partitioning the samples based on GPT’s\nconfidence in its answers, and using different families of LLMs. When GPT answers with\nhigher confidence, the forecasts tend to be more accurate than less confident forecasts. We\nalso find that the earlier version, GPT3.5, shows considerably less impressive performance,\nsuggesting that our main results should not be taken for granted. At the same time, we show\n7We use five years to allow the model’s parameters to change over time, which helps to ensure accuracy.\nWe also experimented with longer windows and found similar results\nFinancial Statement Analysis with Large Language Models 6\nthat the results generalize to other LLMs. In particular, Gemini Pro, recently released by\nGoogle, achieves a similar level of accuracy compared to GPT 4.\nGiven the documented consistently impressive LLM’s performance in fundamental anal-\nysis, it is interesting to understand whythe model is so successful. We examine two broad\nhypotheses. The first hypothesis is that GPT’s performance is driven by its (possibly near-\nperfect) memory. It would be especially problematic if GPT could somehow infer the com-\npany’s identity and year from the data and match this information with the sentiment about\nthis company learned from newspaper articles or press releases. We aim to rule out this\nhypothesis (see Section 6.1). Furthermore, we replicate our results using the most recent\nyear of data, which lies outside GPT4’s training period (i.e., pure out-of-sample tests).\nOur second hypothesis is that GPT generates useful insights based on which the model\ninfers the direction of future earnings. For example, we observe that the model frequently\ncomputes standard ratios computed by financial analysts and, as instructed by CoT prompt,\ngenerates narratives that analyze these ratios. To test this, we pool all narratives generated\nby the model for a given firm-year and encode them into 768-dimensional vectors (embed-\ndings) using BERT. We then feed these vectors into an ANN and train it to predict the\ndirection of future earnings. We find that the ANN trained on the GPT’s narrative insights\nachieves an accuracy of 59%, which is almost as high as the GPT forecast accuracy (60%).\nIn fact, the embedding-based ANN achieves an F1-score that is higher than GPT’s (65%\nvs. 63%). This result presents direct evidence that the narrative insights generated by the\nmodel are informative about future performance. Further, we observe a 94% correlation\nbetween GPT’s forecasts and ANN forecasts based on the GPT’s narratives, suggesting that\nthe information encoded by these narratives is the basis for GPT’s forecasts. We also find\nthat narratives related to ratio analysis, in particular, are most important in explaining\nthe direction of future earnings. In sum, the narratives derived from CoT reasoning are\nresponsible for the model’s superior performance.\nFinally, we explore the economic usefulness of GPT’s forecasts by analyzing their value\nin predicting stock price movements. We find that the long-short strategy based on GPT\nforecasts outperforms the market and generates significant alphas and Sharpe ratios. For\nexample, alpha in the Fama-French three-factor model exceeds 12% per year. GPT stands\nout for doing particularly well in predicting the returns for small companies, as compared to\nANN-based strategies.8\nWe make several contributions to the literature. First, to the best of our knowledge, we\n8This finding aligns with our earlier result that GPT is relatively better in predicting earnings for smaller\ncompanies compared to ANNs. Given that GPT’s training dataset likely contained a disproportionate\namount of information from larger firms, this result further challenges the notion that GPT’s performance\nis merely a function of its memory.\nFinancial Statement Analysis with Large Language Models 7\nare the first to provide large-scale evidence on LLM’s ability to analyze financial statements\n– a complex task that is traditionally performed by human analysts. We show that an LLM\ncan generate state-of-the-art inferences about the direction of the company, outperforming\nfinancial analysts and prior models and generating valuable insights along the way. Im-\nportantly, we show that the language model can successfully analyze numbers in financial\nstatements without any narrative context.\nSecond, our results provide evidence on the limits of LLMs. In particular, the boundaries\nof generative AI to successfully perform tasks outside of their native domain are not well\nunderstood. We find that an LLM excels in a quantitative task that requires intuition\nand human-like reasoning. The ability to perform tasks across domains points towards the\nemergence of Artificial General Intelligence. Broadly, our analysis suggests that LLMs can\ntake a more central place in decision-making than what is previously thought.\nThird, we contribute to the literature on fundamental analysis. Starting from Ou and\nPenman (1989), there is a large literature in accounting that focuses on earnings prediction\nbased on accounting fundamentals (for example, Bochkay and Levine, 2019; Hunt et al.,\n2022; Chen et al., 2022). In particular, Chen et al. (2022) predict the direction of earnings\nchanges using tree-based machine learning models trained on over 12,000 exploratory vari-\nables based on firms’ XBRL tags.9We use a novel approach to analyze financial information\nto derive insights about future performance. In particular, we show that an LLM-based\nfinancial statement analysis, by drawing on vast knowledge and chain-of-thought reasoning,\ncomplements humans as well as specialized models in generating value-relevant information.\nIn that sense, we also contribute to the recent literature on the relative advantage of humans\nversus AI in financial markets (Costello et al., 2020; Liu, 2022; Cao et al., 2024).\n2 Conceptual Underpinnings\nFinancial statement analysis, or fundamental analysis, has long been considered of crit-\nical importance for informed decision-making (e.g., Graham and Dodd, 1934). It uses the\nnumbers reported in financial statements to gain insights into the financial health of the\ncompany, aiming to reveal information about a firm’s future prospects and valuation (Ou\nand Penman, 1989; Piotroski, 2000; Sloan, 2019).\nFinancial statement analysis underlies the work performed by financial analysts, who\nplay a pivotal role in financial markets.10One of their primary tasks involves predicting\n9The observed variation in prediction accuracy relative to Chen et al. (2022) can be attributed to the\nconsiderably fewer predictive variables included in our sample. Additionally, when our analysis is confined\nto firms examined in Chen et al. (2022), the prediction accuracy of GPT notably increases to 64%.\n10Analysts are often formally trained in financial statement analysis. For example, financial statement\nFinancial Statement Analysis with Large Language Models 8\nfirms’ earnings, which serves both as an input in their own stock market recommendations\nand an output that informs investors (Stickel, 1991; Brown et al., 2015). When making\nearnings forecasts, their work typically begins with a systematic analysis of financial state-\nments (Bouwman et al., 1987), often using standardized templates to ensure consistency and\naccuracy. This analysis enables financial analysts to establish a baseline understanding of\na company’s financial position and performance, assessing factors such as operating perfor-\nmance or capital structure. They then contextualize this financial data by drawing upon\ntheir industry and private knowledge about the firm before issuing their forecasts (Brown\net al., 2015). The accuracy and quality of these forecasts not only drive market percep-\ntions but also are fundamental to analysts’ career advancement and job security (Basu and\nMarkov, 2004; Groysberg et al., 2011).\nPrior research generally concludes that sell-side analysts outperform time series models in\nterms of producing credible annual earnings forecasts (e.g., Bradshaw, 2011). Consequently,\nthese forecasts are frequently used as a proxy for markets’ earnings expectations. At the same\ntime, prior research has shown that financial analysts produce potentially erroneous or biased\nestimates (Bradshaw, 2011; Kothari et al., 2016). For example, Green et al. (2016) show that\nanalysts make technical errors and questionable economic judgments when evaluating firms\nwith quantitative methods. Evidence from De Bondt and Thaler (1990) or Bordalo et al.\n(2019) suggest that financial analysts overreact to recent events. These mistakes and biases\nhighlight the complexity of processing information efficiently when large volumes of data are\ninvolved.\nRecognizing these challenges in conventional financial forecasting and human information\nprocessing, general-purpose language models, such as ChatGPT, hold promise in facilitat-\ning financial statement analysis and the associated tasks such as earnings forecasting and\ndecision-making more generally. These advanced AI systems are noted for their expansive\nknowledge across various domains and ability to quickly and efficiently process large quanti-\nties of data (Achiam et al., 2023). For example, their proficiency extends to answering CFA\nor CPA exam questions (Eulerich et al., 2023), demonstrating their financial knowledge and\npotential for understanding theories. In a similar vein, prior literature has shown that these\nmodels are capable of efficiently processing large sets of financial data (e.g., Kim et al.,\n2023b,a). LLMs have also shown promise in predicting certain economic outcomes. Lopez-\nLira and Tang (2023) and Jiang et al. (2022) show that GPT can explain short-term stock\nreturns based on newspaper headlines and Bybee (2023) finds that GPT’s macroeconomic\nprediction aligns well with the expert survey results. In addition, Hansen and Kazinnik\n(2023) document that GPT can understand the political stance of FOMC announcements\nanalysis is a major part of the Level I CFA exam.\nFinancial Statement Analysis with Large Language Models 9\nand relate it to future macroeconomic shocks.\nHowever, despite the successes of large language models in many tasks, they are pri-\nmarily viewed as a support tool and their ability to act autonomously to perform financial\nstatement analysis at a level of a human analyst faces significant challenges. First, financial\nstatement analysis is a broad task that is more of an art than science, whereas machines\ntypically excel in narrow, well-defined tasks. It requires common sense, intuition, ability\nto reason and make judgements, ability to handle situations unseen previously. Second,\nLLM is not trained to analyze financial information, e.g., in the same way they are trained\nto summarize text or answer questions. In fact, inputs into the tasks performed by LLMs\nhave been predominantly qualitative and language-based, and, LLMs have struggled with\nunderstanding numeric domain (Brown et al., 2020). Third, humans are more capable of\nincorporating their knowledge of broader context – something a machine often cannot do –\nby taking into account soft information, knowledge of the industry, regulatory, political, and\nmacroeconomic factors. These factors stack up against the odds that an LLM can achieve a\nhuman like performance in analyzing financial statements.11\nAn alternative to utilizing a general-purpose large language model for financial statement\nanalysis involves specifying a more narrow objective, such as earnings prediction, and training\na specialized ML model, such as Artificial Neural Network (ANN), to perform this task.\nUnlike the general-purpose large language models, which are trained to predict the next word\nin a textual sequence, ANNs learn deep interactions among a large number of predictors to\ndeliver powerful forecasts of the target variable.12Because LLMs are not trained to uncover\nthese complex relationships among predictors, they are fundamentally disadvantaged relative\nto the specialized models in a specific prediction task. Nevertheless, the effectiveness of these\nANNs can be limited if they encounter patterns not observed during training with sufficient\nfrequency. This is where theoretical knowledge or general understanding of how the world\nworks becomes essential, as does the value of human experience, intuition, and judgment.\nThis grants possibly an important advantage to an LLM due to its training on a vast body of\ngeneral knowledge that encompasses a multitude of business cases and situations, financial\ntheories, and economic contexts. This broader theoretical foundation potentially allows\nLLMs to infer insights even from unfamiliar data patterns, providing an advantage in the\ncomplex domain of financial analysis.\n11These more complex quantitative tasks have been traditionally seen as outside of the LLM’s “technolog-\nical frontier” (e.g. Dell’Acqua et al., 2023). Consistent with this argument, Li et al. (2023) processes earnings\npress releases and finds that GPT performs worse in predicting earnings relative to sell-side analysts.\n12Over time, methods for predicting earnings have progressively advanced within the accounting litera-\nture. Ou and Penman (1989) predict earnings changes using a stepwise logistic regression model that uses\napproximately 60 accounting variables as input. Most recently, Chen et al. (2022) use 13,881 in-line XBRL\ntags and tree-based machine learning models to predict future earnings.\nFinancial Statement Analysis with Large Language Models 10\n3 Methodology and Data\nIn this section, we outline how we approach the primary task of using an LLM to analyze\nand predict earnings changes. Earnings prediction is a complex task that combines qualita-\ntive and quantitative analyses and involves professional judgment. We model how analysts\nmake earnings predictions with a chain-of-thought prompt using GPT 4.\n3.1 Financial Statement Analysis and Earnings Prediction\nOverview Earnings prediction derived from financial statement analysis is of considerable\nimportance to accounting information users. For example, such predictions help investors\nto make inferences about the cross-section of expected stock returns (Fama and French,\n2015) or to pick the best-performing stocks (Piotroski, 2000). However, earnings are hard\nto predict as they are influenced by many exogenous factors such as macroeconomic shocks\n(Ball et al., 2022), product market demand shocks, changes in accounting standards (Ball\net al., 2000), and many other factors. Therefore, predicting earnings is challenging even for\nstate-of-the-art ML models (see Bochkay and Levine, 2019; Chen et al., 2022, for example).\nFinancial analysts approach this complex task by performing financial statement analysis.\nThey first analyze financial statements, identifying notable changes or trends in accounting\ninformation. They choose which financial ratios to compute to obtain further insights. Their\nanalysis is enriched by contextual information, such as industry information, understanding\nof the competitive landscape, and macroeconomic conditions (Bouwman et al., 1987). Based\non this information, they apply professional judgments to determine whether a company’s\nearnings will grow or contract in the future.\nIn this study, we specifically focus on a relatively narrow information set that includes\nnumerical information reported on the face of two primary financial statements. While\nthis lacks textual information or broader context and thus puts an LLM at a disadvantage\nrelative to a human, it presents a well-defined information set of exclusively numeric data.\nThis approach allows us to test the limits of the model when analyzing financials and deriving\ninsights from the numeric data – something that an LLM is not designed nor trained to do.\nTo approach FSA-based earnings prediction based on a Large Language Model, we im-\nplement two types of prompts. First, we use a “simple” prompt that instructs an LLM to\nanalyze the two financial statements of a company and determine the direction of future\nearnings. This prompt does not provide further guidance on how to approach the prediction\ntask, however.13Second, we implement a Chain-of-Thought prompt that breaks down the\n13In particular, we simply present a standardized and anonymous balance sheet and income statement\nand ask the model to predict whether earnings will increase or decrease in the subsequent period.\nFinancial Statement Analysis with Large Language Models 11\nproblem into steps that parallel those followed by human analysts. This prompt effectively\ningrains the methodology into the model, guiding it to mimic human-like reasoning in its\nanalysis. We mostly focus on the results from this second prompt in our analysis.\nHuman Processing and Chain-of-Thought Modern large language models can retrieve\nnumbers from structured tables and perform simple calculations. However, they lack the\nability to reason like a human and perform judgment. Recent research suggests that chain-\nof-thought prompting can significantly enhance the reasoning and problem-solving abilities\nof large language models (Wei et al., 2022).\nWe implement the CoT prompt as follows. We instruct the model to take on the role\nof a financial analyst whose task is to perform financial statement analysis. The model\nis then instructed to (i) identify notable changes in certain financial statement items, and\n(ii) compute key financial ratios without explicitly limiting the set of ratios that need to be\ncomputed. When calculating the ratios, we prompt the model to state the formulae first, and\nthen perform simple computations. The model is also instructed to (iii) provide economic\ninterpretations of the computed ratios. Then, using the basic quantitative information and\nthe insights that follow from it, the model is instructed to predict whether earnings are likely\nto increase or decrease in the subsequent period. Along with the direction, we instruct the\nmodel to produce a paragraph that elaborates its rationale. Overall, this set of instructions\naims to replicate how human analysts analyze financial statements to determine whether a\nfirm’s performance is sustainable (Bouwman et al., 1987).\nIn addition to the binary prediction accompanied by a rationale statement, we also\nprompt the model to provide the predicted magnitude of earnings change and the confidence\nin its answer (Bybee, 2023; Choi and Kim, 2023). The magnitudes contain three categories:\nlarge, moderate, and small. The confidence score measures how certain the model is in\nproducing its answers and ranges from zero (random guess) to one (perfectly informed).\nWe use gpt-4-0125-preview , which is the most updated GPT model by OpenAI at\nthe time of our experiment. The temperature parameter is set to zero to ensure minimal\nvariability in the model’s responses. We do not specify the amount of maxtokens , and\ntop-p sampling parameter is set to one (i.e., the most likely word is sampled by the model\nwith probability one). In addition, we enable the logprobs option to obtain token-level\nlogistic probability values. Figure 1 provides a visual illustration of GPT’s processing steps.\n3.2 Data\nWe use the entire universe of Compustat annual financial data from the 1968 to 2021\nfiscal years. We also set aside data for 2022 to predict 2023 fiscal year earnings to test for\nFinancial Statement Analysis with Large Language Models 12\nthe robustness of the model’s performance outside GPT’s training window. In particular,\nthe GPT-4-Turbo preview’s training window ends in April 2023, and the model cannot have\nseen the earnings data of 2023, which was released in late March 2024. Following prior\nliterature, we require that each observation has non-missing total assets, year-end assets\nvalue exceeding one million dollars, a year-end stock price exceeding one dollar per share,\nand a fiscal period end date of December 31.14We also drop observations where the balance\nsheet equation does not hold. These filters leave us with 150,678 observations from 15,401\ndistinct firms, reasonably approximating the Compustat universe.\nFor each firm-year, we reconstruct the balance sheet and income statement using the\ndata from Compustat. The format follows Capital IQ’s balancing model and is the same\nacross all firm years. We omit any identifying information, such as the firm name or dates of\nthe financial statements. This step ensures that all firm-year observations have an identical\nfinancial statement structure. Consistent with US GAAP reporting requirements, we provide\ntwo years of balance sheet and three years of income statement data. An example of the two\nstatements is provided in Appendix B.15\nFor the analysis that involves analyst forecasts, we use data from IBES, starting the\nsample in 1983. We extract individual forecasts and construct monthly consensus forecasts.\nThis analysis restricts the sample to firm-years with analyst following. We require that each\nobservation has at least three analyst forecasts issued, which leaves us with 39,533 firm-year\nobservations.\nWe report descriptive statistics for the variables used in our analyses in Table 1. Panel A\ndescribes the full sample (1968-2021), and Panel B is restricted to the analyst sample (1983-\n2021). The data in Panel A reveals that approximately 55.5% of observations report an actual\nincrease in earnings ( Target ). Predicted values include the prefix “ Pred ” and vary depending\non the model. For example, GPT prediction ( PredGPT ) implies that, on average, 53.0% of\nobservations will experience an increase in earnings. In Panel B, PredAnalyst 1mdenotes\nthe forecasts issued within one month from the previous year’s earnings release. Analyst\nforecasts indexed by 3m and 6m suffixes are defined in an analogous manner. Compared to\nGPT, financial analysts tend to be slightly more pessimistic in their forecasts (fluctuating\naround 52% depending on the timing of the forecasts). Panel B also reveals that companies\nin the Analyst Sample are, on average, larger in size ( Size), have a lower book-to-market\nratio ( BtoM ), higher leverage ( Leverage ), and lower earnings volatility ( Earn Vol). However,\nthey are similar in terms of the actual frequency of EPS increases.\n14Focusing on December 31 firms allows for more straight-forward asset pricing tests in Section 7 and is\nconsistent with Ou and Penman (1989); Hunt et al. (2022).\n15Importantly, we do not train or fine-tune the LLM model on the financial statements. The model\nobserves only a single balance sheet and income statement at a time, as provided in Appendix B.\nFinancial Statement Analysis with Large Language Models 13\n4 How Does an LLM Perform Compared to Financial Analysts?\nIn this section, we evaluate the performance of a large language model in the analysis\nof financial statements aimed at predicting the direction of future earnings by using human\nanalysts as a benchmark. All prediction models have a binary target variable, which indicates\nan increase or decrease in EPS in the subsequent year.\n4.1 Prediction Methods and Evaluation Metrics\nNaive model First, as a naive benchmark, we assume that the directional change in\nearnings will stay the same. In particular, if EPS has increased (decreased) in year trelative\nto yeart−1, the naive prediction for year t+ 1 is also “increase” (“decrease”).\nAnalysts’ forecasts We use a consensus analyst forecasts of year t+ 1 EPS published\nfollowing the announcement of year tearnings. If there are multiple forecasts issued by a\nsingle analyst, we use the closest one to the year tearnings release dates. This approach\nhelps us to ensure that human analysts are making predictions of one-year-ahead earnings\nbased on financial statements published in the current year. Then we take the median value\nof analysts’ forecasts and compare it to the actual year tEPS. We require at least three\nanalyst forecasts in a given firm-year to compute median values. If the median forecasted\nEPS value is larger than the year tEPS, we label the prediction as “increase” and vice versa.\nAnalyst forecast accuracy is then obtained in an analogous manner.\nAs a comparison, we also collect analyst forecasts issued at least three and six months\nafter the release of year tfinancial statements. This ensures that the analysts have enough\ntime to process the reported financials. However, this also means that the analysts will\nhave access to one or two quarterly financial statements and other contextual information\ngenerated during the year t+ 1. Therefore, human analysts generally have an informational\nadvantage relative to the models that rely on time tinformation only.\nEvaluation Metrics We report two common metrics to evaluate the quality of the pre-\ndiction method: accuracy and F1-score. Accuracy is the percentage of correctly predicted\ncases scaled by the total number of predictions made. F1-score is the harmonic mean of\nprecision and recall. Precision measures the proportion of true positive predictions in the\ntotal positive predictions, while recall measures the proportion of true positive predictions\nout of all actual positives. In particular, F1-score is defined as follows:\nF1 =2×TP\n2×TP+FP+FN(1)\nFinancial Statement Analysis with Large Language Models 14\nwhereTPis the number of true positive predictions, FPis the number of false positive\npredictions, and FNis the number of false negative predictions.\n4.2 Main Results\nTable 2 compares GPT’s prediction accuracy with that achieved by financial analysts.\nBased on the first-month forecast following the release of prior year financial statements,\nanalysts’ accuracy is 52.71% and F1 score is 54.48% when predicting the direction of one-\nyear-ahead earnings. As expected, this is better than predictions based on a naive model\n(accuracy = 49.11% and F1 score = 53.02%). However, these results also reiterate the\nnotion that changes in earnings are very hard to predict, even for sophisticated financial\nanalysts. As expected, the analysts’ prediction accuracy improves through the course of\nthe yeart+ 1, achieving an accuracy of 55.95% and 56.58% for month-three and month-six\nforecasts, respectively.\nTurning to GPT’s predictions, we observe the following: Using a simple prompt instruct-\ning GPT to analyze financial statements and predict the direction of future earnings yields an\naccuracy of 52.33% and an F1-score of 54.52%. Thus, without CoT reasoning, the model’s\nperformance is on par with the first-month consensus forecasts by financial analysts, fol-\nlowing the earnings release. However, the performance markedly improves when we utilize\nCoT-based GPT forecasts. With chain-of-thought prompts, GPT achieves an accuracy of\n60.35%, or a 7 percentage points increase compared to analyst predictions one month after\nthe earnings release. The difference is statistically significant at 1% level.16This edge is\nparticularly noteworthy since we do not provide to the language model any available to the\nanalysts narrative or contextual information beyond the balance sheet and income statement.\nTaken together, our results suggest that GPT can outperform human analysts by per-\nforming financial statement analysis even without any specific narrative contexts. Our results\nalso highlight the importance of a human-like step-by-step analysis that allows the model to\nfollow the steps typically performed by human analysts. In contrast, simply instructing the\nmodel to analyze complex financial statements does not yield strong prediction results.\n4.3 Complementarity Between Human Analysts and GPT\nGiven that GPT outperforms human analysts in predicting future earnings, this finding\nraises the question of whether an LLM can largely replace human analysts. In our context,\nhumans are expected to rely on a broader information set and hence should have an advantage\nover an LLM that does not have access to qualitative information, for example. More\n16GPT outperforms human analysts in terms of accuracy under 5% statistical significance.\nFinancial Statement Analysis with Large Language Models 15\ngenerally, humans often rely on soft information not easily accessible to a machine (Costello\net al., 2020; Liu, 2022), which puts humans at an informational advantage. We next explore\nthe presence of complementarities and trade-offs related to LLM vs. human forecasts.\nSources of Incorrect Answers We start with the analysis of instances where forecasts are\nerroneous. We estimate a simple linear regression to examine whether firm characteristics\nhave systematic associations with prediction accuracy. I(incorrect = 1) is an indicator\nvariable that equals one when the earnings prediction does not match the actual change in\nearnings. We then estimate the following OLS regression:\nI(incorrect = 1) it=βXit+δyear+δind+εit (2)\nXitis a vector of firm i’s yeartcharacteristics: asset size, leverage, book-to-market ratio,\nearnings volatility, loss indicator, and property, plant, and equipment scaled by total assets.\nδyearandδinddenote year and industry (SIC two-digit) fixed effects, respectively. All con-\ntinuous variables are winsorized at the 1% level and standard errors are clustered at the SIC\ntwo-digit industry level.\nWe present the results in Table 3, Panel A, and Figure 2. In column (1), we document\nthat GPT’s predictions are more likely to be inaccurate when the firm is smaller in size,\nhas a higher leverage ratio, records a loss, and exhibits volatile earnings. These results are\nintuitive and, notably, prior studies find these characteristics to be economically associated\nwith earnings quality.17For comparison, in columns (2), (3), and (4), we report the determi-\nnants of analysts’ inaccurate predictions. Several interesting differences emerge compared to\ncolumn (1). First, even though analysts face difficulties in predicting small firms’ earnings,\nthe magnitude of these coefficients is nearly half compared to the coefficient in column (1)\n(p-value is less than 1% for all three comparisons). Considering that analysts have access to\nnarrative information and broader context, this result is consistent with Kim and Nikolaev\n(2023b), who show that context matters more for prediction tasks when the firm is smaller\nin size. Another notable difference is that analysts are less likely to make errors relative to\n17Due to high fixed costs of maintaining adequate internal controls, small firms may have lower-quality\naccounting earnings (Ball and Foster, 1982; Ge and McVay, 2005) and are more likely to restate their\nearnings in subsequent periods (Ashbaugh-Skaife et al., 2007). High leverage ratios are often indicative of\nfirms being closer to debt covenant violations. Such firms might be more incentivized to engage in earnings\nmanagement to meet or beat financial thresholds, leading to lower-quality earnings (Watts and Zimmerman,\n1986). Also, when firms experience unusual financial circumstances such as reporting losses, analysts tend to\nperform worse than average (Hwang et al., 1996; Hutton et al., 2012). Lastly, Donelson and Resutek (2015)\ndocument that past volatility of earnings is negatively associated with its predictive power. Considering that\nGPT only uses numerical financial information as its input, these results align well with Kim and Nikolaev\n(2023a,b) that contextual information becomes relatively more important when firms experience losses and\ntheir size is small.\nFinancial Statement Analysis with Large Language Models 16\nGPT when a firm reports a loss and exhibits volatile earnings. These findings are the same\nfor all analyst forecast measures as the magnitudes of the coefficients on Loss andEarn-\nings Volatility in columns (2), (3), and (4) are consistently smaller than that of column (1).\nTaken together, our results show that analysts and GPT both have difficulties in predicting\nthe earnings of small, loss-reporting firms. However, analysts tend to be relatively better\nat dealing with these complex financial circumstances than GPT, possibly due to other soft\ninformation and additional context (Costello et al., 2020).\nIncremental Informativeness We next test whether analysts’ forecasts, despite lower\naccuracy, add useful insights incremental to GPT’s predictions. We regress an indicator\nI(Increase = 1), which equals one when subsequent period earnings increase and zero\notherwise, on the direction of future earnings predicted by GPT and/or analysts. Specifically,\nwe estimate the following OLS regression:\nI(Increase = 1) it=β1PredGPT it+β2PredAnalyst it+δyear+δind+εit (3)\nwherePredXis an indicator that equals one when ” X” (which is either “GPT” or “An-\nalyst”) predicts an increase in earnings, and zero otherwise. δyearandδindare year and\nindustry (SIC two-digit level) fixed effects. Standard errors are clustered at the industry\nlevel.\nThe results are presented in Table 3, Panel B. In column (1), we find that GPT’s predic-\ntion, on a standalone basis, is positively associated with future outcomes while controlling\nfor industry and year-fixed effects. The same result holds for individual analysts’ forecasts\nas can be seen in columns (2), (3), and (4). Consistent with the results in Table 2, analysts’\nforecasts issued six months after the earnings release exhibit stronger associations with the\nactual outcomes than the forecasts issued one month after the earnings release (the adjusted\nR-squared in column (4) is 0.044, which is almost twice the adjusted R-squared value in\ncolumn (2)).\nIn columns (5), (6), and (7), we include both GPT and analyst forecasts simultaneously\nin a single regression. Across all models, both coefficients are statistically significant. We\nobserve that the coefficient on GPT is largely unchanged (its t-statistics marginally decreases\nfrom 2.99 to 2.67) and the coefficient on analysts’ predictions increases in magnitude when\nboth variables are used simultaneously (e.g., from 0.073 in column (2) to 0.110 in column (5)).\nThe adjusted R-squared value also increases from 0.070 in column (1) to 0.089 in column\n(5). These results indicate that GPT and human analysts are complementary, corroborating\nFinancial Statement Analysis with Large Language Models 17\nour results in Table 3.\nDoes GPT Do Well When Humans Struggle? To explore the relative advantage of\nan LLM compared to human analysts, we examine instances when human analysts are likely\nto struggle with accurately forecasting earnings. In particular, we identify instances where\nanalyst forecasts are likely to be biased or inefficient ex ante. We also consider instances in\nwhich analysts tend to disagree about future earnings (exhibit dispersion).\nTo estimate ex-ante bias (inefficiency) in analysts’ forecasts, we run cross-sectional re-\ngressions of analyst forecast errors on the same firm characteristics as in Equation 2. We\nthen take the absolute value of the fitted values from this regression.18Consistent with prior\nliterature, forecast errors are defined as the difference between actual EPS and forecasted\nEPS, scaled by the stock price at the end of the last fiscal year. In addition to ex-ante bias,\nwe measure the disagreement in analysts’ forecasts. Specifically, we use the standard devia-\ntion of analysts’ forecasted EPS values, scaled by the stock price at the end of the preceding\nfiscal year.\nWe then partition the sample based on the quartile values of analyst bias and estimate\nEquation 3 for each group. The results are presented in Panel C of Table 3. By comparing\nthe coefficients in columns (1) and (2), we observe important differences. When the analysts’\nbias is expected to be relatively low, GPT’s predictions receive a smaller weight (compared\nto that in column (2) when the bias is expected to be higher), and the coefficient on analysts’\npredictions is relatively large. These differences are statistically significant at the 1% level.\nThey suggest that GPT is more valuable in situations when human analysts are likely to\nbe biased. Similar results follow in columns (3) and (4) when we partition the sample on\nanalyst disagreement: GPT’s prediction receives more weight when analysts’ disagreement\nis high and vice versa.\nTaken together, our results indicate that GPT’s forecasts add more value when human\nbiases or inefficiencies are likely to be present.\n5 Comparison with Specialized ML Models\nSo far, we have shown that GPT’s predictions largely outperform human analysts. As\nhuman analysts are known to have a systematic bias in their forecasts, we raise the bar and\nturn to more sophisticated benchmarks, including state-of-the-art machine learning models.\n18Note that errors should be unpredictable if forecasts are unbiased and efficient.\nFinancial Statement Analysis with Large Language Models 18\n5.1 Methodology\nFollowing Ou and Penman (1989) and Hunt et al. (2022), we focus on 59 financial vari-\nables obtained from the Compustat Annual database to predict future earnings but exclude\nthe price-to-earnings ratio for consistency reasons (stock price is not financial statement in-\nformation). We perform two different prediction exercises: stepwise logistic regression and\nANN. In both cases, we use a rolling five-year training window. That is, we estimate (train)\nthe model using data from years t−5 tot−1, and apply the trained model to the year t\ndata to generate forecasts. By doing so, we ensure that the models do not learn from the\ntest data during the training phase. Since our sample spans from fiscal year 1962 to 2021,\nwe train 56 distinct models for each prediction method.\nIn the stepwise logistic regression, we follow Ou and Penman (1989) and only retain the\nsignificant variables from the first step when performing the second step of the procedure.\nThe trained logistic regression then yields a probability value instead of a binary variable as\nits output. We classify observations with a probability value higher than 0.5 as an increase\n(and a decrease otherwise). In contrast to the logistic regression, the ANN model allows\nfor non-linearity among the predictors. Our model has an input layer with 59 neurons, two\nhidden layers with 256 and 64 neurons each, and an output layer with two neurons (Kim\nand Nikolaev, 2023a). The output layer produces a two-dimensional vector ( p1,p2), and we\nclassify the outcome as an increase when p1> p 2and vice versa. We use Adam optimizer,\nReLU activation function, and cross-entropy loss. We use batch training with a batch size of\n128. All input variables are standardized. Missing continuous variables are imputed as the\nyear-industry average. We apply early stopping criteria with a patience of five epochs, which\nindicates that the model stops training when there is no improvement in performance for\nfive consecutive epochs.19For each training phase, we assign a random 20% of the training\nsample to the validation set and optimize the learning rate and dropout rate. Specifically,\nwe perform a grid search of nine iterations, using three learning rates (1 e−5,1e−3,and 1e−1)\nand three dropout rates (0, 0.2, and 0.4).\n5.2 Main Results\nOverall Results We report the results in Table 4, Panel A, and Figure 3. Stepwise logistic\nregressions following Ou and Penman (1989) achieve an accuracy of 52.94% and an F1 score\nof 57.23%. We observe a considerably higher prediction accuracy using the ANN model.\nThe model achieves a 60.45% accuracy and an F1-score of 61.62%. This result highlights the\nimportance of non-linearities and interactions among financial variables for the predictive\n19The maximum allowed training epochs is set to 50 yet none of the models hit this limit.\nFinancial Statement Analysis with Large Language Models 19\nability of numerical information.\nConsistent with the results in the analyst sample, our CoT-based GPT predictions achieve\nan accuracy of 60.31%, which is on par with the specialized ANN model. In fact, in terms\nof the F1-score, GPT achieves a value of 63.45%, which is the highest among all prediction\nmethods. This indicates a remarkable aptitude of GPT to analyze financial statements.20\nNot only does it outperform human analysts, but it generates performance on par with the\nnarrowly specialized state-of-the-art ML applications.\nWe further examine the possibility that ANN versus GPT performance is partly driven\nby the slightly different input variables: we use balance sheet and income statement variables\nfor GPT, but 59 Ou and Penman (1989) ratios for ANN. Thus, to ensure that the results\nare not an artifact of this choice, we also train an ANN model using the same balance sheet\nand income statement variables. We scale balance sheet items by total assets and income\nstatement items by total sales. We also include change in revenue, change in lagged revenue,\nchange in total assets, and revenue scaled by total assets. The ANN model with financial\nstatement information achieves an accuracy of 60.12% and an F1-score of 61.30%, which are\nslightly lower than those of GPT.\nTime Trends We report the overall time trend of GPT’s and ANN’s prediction accuracy\nin Figure 4 (detailed annual accuracy and F1-scores are reported in Appendix A). The left\npanel shows a negative time trend in GPT’s prediction accuracy. In terms of the economic\nmagnitude, GPT’s accuracy has decreased, on average, by 0.1% point per year, which trans-\nlates into a decrease in accuracy by 5.4 percentage points over the 54-year sample period.\nInterestingly, we observe sharp drops in prediction accuracy in 1974, 2008-2009, and 2020.\nThese periods overlap with international macroeconomic downturns: the oil shock in 1974,\nthe financial crisis in 2008-09, and the Covid-19 outbreak in 2020. This result is comforting\nas GPT should not foresee unexpected, exogenous macroeconomic shocks if its performance\nis unrelated to memory.21Most importantly, in the right panel of Figure 4, we plot the\ntime-series trend of the “difference” in the accuracy of GPT and ANN models. ANN models\nexhibit similar time trends compared to GPT with their annual differences fluctuating close\nto zero. Thus, for both evaluation metrics, we find a negative and statistically significant\ntime trend, implying that it has become increasingly difficult to predict future earnings using\nonly numeric information.22\n20GPT outperforms stepwise logistic predictions at 1% level. However, the difference between GPT and\nANN performance is not statistically significant at conventional levels.\n21We discuss this potential issue more formally in Section 6.1.\n22This result corroborates Kim and Nikolaev (2023a), who find that the informational value of narrative\ncontext in predicting future earnings has increased over time.\nFinancial Statement Analysis with Large Language Models 20\nSources of Inaccuracy Next, we explore which firm characteristics are associated with\nthe likelihood of making incorrect earnings predictions. Column (1) of Table 4 focuses\non the accuracy of GPT’s predictions and is consistent with our findings for the analyst\nsample (Table 3). We then report the determinants of the incorrect predictions by ANN\nand logistic regression models in columns (2) and (3), respectively. Both the ANN and\nlogistic regression are also more likely to generate inaccurate predictions when firms are\nsmaller, have higher leverage, record a loss, and have higher earnings volatility. However,\ninterestingly, ANN is relatively more likely than GPT to make inaccurate predictions when\nfirms are smaller and record a loss. A one standard deviation decrease in firm size reduces\nGPT’s prediction accuracy by 3.4 percentage points. In contrast, the same change in firm\nsize is associated with a 5.5 percentage point decrease in prediction accuracy for the ANN\nmodel. The difference between the two coefficients is statistically significant at the 1% level.\nSimilarly, the coefficients on Loss andEarnings Volatility are statistically different at the\n5% level. The differences between logistic regression and GPT predictions are even more\npronounced. These findings hint at the ability of GPT to make better predictions for less\ncommon data patterns (e.g., loss-making firms), presumably due to its ability to rely on its\nconceptual knowledge and theoretical understanding of business.\nIncremental Informativeness While GPT’s performance is comparable to that of an\nANN, we also examine whether GPT conveys incremental information when compared to\nspecialized ML models. This analysis is reported in Panel C. In columns (1) to (3), we show\nthat across all models, predicted earnings changes, individually, are positively associated\nwith the actual changes. In column (4), when both GPT and ANN forecasts are included\nsimultaneously, both remain statistically significant and hence contain incremental informa-\ntion. Interestingly, the coefficient on ANN becomes one-third in magnitude (compared to\ncolumn (2)) and its statistical significance deteriorates (from a t-statistic of 3.69 to 2.36),\nwhereas the coefficient on GPT remains stable. This result suggests that GPT captures some\nadditional dimensions of information than non-linear interactions among financial variables\nwhen predicting future earnings, e.g., external theoretical knowledge.\n5.3 Confidence, Magnitude, and Generalizability\n5.3.1 LLM’s Confidence\nMethod We estimate the confidence of LLM’s answers based on two methods. First, we\nexplicitly instruct the model to report a confidence score on its earnings prediction, with one\nbeing perfect confidence and zero being a pure guess (Bybee, 2023). Second, we compute\nFinancial Statement Analysis with Large Language Models 21\nan alternative confidence score based on token-level logistic probability values, which we\ndirectly take from the probability vector provided by the model. Specifically, we average the\nlogistic probability values across all output tokens to measure the overall certainty of the\nmodel answer.\nResults For both approaches, we report prediction results of the high confidence (fourth\nquartile) and the low confidence (first quartile) groups. We present the results in Figure 5 and\ncolumns (1) to (4) of Table 5. The model performs better when it reports greater confidence.\nIn the high confidence group, the model achieves an average accuracy of 62.44% (63.15%)\nbased on the reported confidence value (confidence score derived from logistic probabilities),\nwhich is approximately 2.6 (4.6) percentage points higher than the corresponding accuracy\nof the low confidence group. We find similar results based on the F1 score. Overall, this\nresult indicates that the model is capable of distinguishing between instances where earnings\nare more predictable.\n5.3.2 Magnitude\nMethod Recall that we also instruct the model to provide the expected magnitude of\nearnings change: “large”, “moderate”, or “small. ” As in Ou and Penman (1989) and Hunt\net al. (2022), we expect the model to be more accurate in determining the directional change\nwhen it predicts large rather than immaterial changes.\nResults We present the results in Figure 5 and columns (5) and (6) of Table 5. We find that\nthe average accuracy is 62.03% when the model predicts large changes whereas it decreases\nto 60.22% for small changes. We document a similar pattern for F1 scores: 61.16% for large\nchanges vs. 57.95% for small changes. Overall, when the model expects a larger change, its\ndirectional predictions are more accurate.\n5.3.3 LLM type\nMethod We also test whether the capabilities associated with a specific LLM type deter-\nmine its predictive ability. In the main analysis, we use the most recent version of GPT,\nGPT-4-turbo. We also experimented with a less powerful LLM version from the same fam-\nily, GPT-3.5-turbo, and otherwise used the same experimental settings. In addition, we also\nexplored another family of LLMs provided by Google, namely, Gemini Pro 1.5 (also with\nthe same experimental settings). Due to considerable processing time, we choose a random\n20% sample for this set of analyses.\nFinancial Statement Analysis with Large Language Models 22\nResults We present the results in Figure 5 and Table 5, columns (7) to (9). GPT 4\nachieves the best performance, followed by Gemini 1.5, and GPT 3.5. Gemini 1.5 achieves\nan overall accuracy of 59.15%, which is close to that of GPT 4 (61.05%) in the same 20%\nsample. However, GPT 3.5 achieves an accuracy of only 52.29% and an F1-score of 59.17%,\nwhich are all substantially lower than our GPT 4 benchmarks. We also find that the outputs\nof GPT 4 and Gemini 1.5 are largely overlapping with only 1,808 out of 30,135 firm-years\n(approximately 6%) having opposing predictions. Overall, this analysis suggests that our\nfindings are not confined to a specific family of LLMs. Although the final prediction results\nlargely rely on the performance of the backbone language model, recent generations of LLMs\nare capable of analyzing financial statements and making informed decisions.\n6 Where Does an LLM’s Predictive Ability Come From?\nIn this section, we aim to understand the sources of GPT’s predictive ability. We explore\ntwo broad explanations. The first explanation is that GPT’s performance comes from its\nmemory, e.g., due to the model’s ability to identify the company based on numeric data.\nWe aim to rule out this possibility as it undermines the integrity of the model’s predictions.\nAnother explanation is that the strength of the model is in its ability to generate narrative\ninsights based on its analysis of numeric data. We explore each of these possibilities next.\n6.1 Is There a Look-ahead Bias in the Model?\nAn important concern with the reliance on a pre-trained large language model in a\nprediction task is its potential for a look-ahead bias (e.g. Sarkar and Vafa, 2024). For\nexample, the model may have been trained on the company-specific financial data and,\nhence, already may “know” the answer as to whether earnings increased or decreased in the\nfuture (or have a general sense of how well the company did over time). Our research design\nis relatively immune from this potential bias (e.g. Glasserman and Lin, 2023) because we use\na consistent anonymized format of financial statements across firms. This makes it virtually\nimpossible for the model to infer a firm’s identity from the structure of financial statements\nor specific account names. We also ensure the statement does not contain any dates and use\nrelative years, i.e., tort−1. This later mitigates the concern that the model has knowledge\nabout macroeconomic trends in a specific year and uses it to predict future earnings. To\nappreciate this issue, imagine that the model was able to match a given set of financials to\n2007. In this case, the model could draw on its knowledge of the major economic downturn\nin 2008 and adjust its prediction accordingly.\nEven though the anonymous nature of financial statements should prevent the model from\nFinancial Statement Analysis with Large Language Models 23\n“guessing” the entity, we perform two formal analyses to further rule out this concern.23\nCan GPT Guess Firm Name and Year? In this set of tests, we instruct the model\nto make guesses about the firm or year based on the financial statements that we provide.\nSpecifically, we ask the model to provide the ten most probable firm names and the most\nprobable fiscal year. Additionally, we force the model to produce outputs even when it\nbelieves that it cannot make any informed guess.\nFor economic reasons, our first set of experiments does not include any chain-of-thought\nprompts. We perform this experiment on 10,000 random observations. The results are\npresented in Table 6, Panel A. We find that the model correctly identifies the firm name\nwith an accuracy of 0.07%, which is lower than the accuracy of a random guess from the\npopulation of names in our data. In Figure 7 left panel, we plot the ten most frequently\nproduced firm names. We find that the model almost always predicts the same set of ten\nfirms, including Tesla, Facebook, and Amazon. This result is consistent with the model’s\ntraining objective to produce the most probable words (name in this case) conditional on its\ninformation. Absent an informative prior, the model is likely to predict the most visible or\npopular firms in its training corpus.\nThe accuracy of correctly guessing the year of financial statements is 2.95%. In the right\npanel of Figure 7, we plot the actual fiscal year and GPT’s prediction in one plane. We\nobserve that almost all predictions are 2019, 2020, or 2021 independent of the actual year,\nwhich is inconsistent with the model’s ability to guess the year.24\nIn the second set of experiments, we use the exact same chain-of-thought prompts as\nin the main analysis, but then ask the model to guess the firm name and year (instead of\npredicting earnings). We use a random sample of 500 observations. Panel B of Table 6\ncontains the results. The findings confirm very low accuracy and thus address a potential\nconcern that the CoT prompt is more capable of invoking the model’s memory. Taken\ntogether, our results strongly suggest that the model cannot make a reasonable guess about\nthe entity or the fiscal year from the anonymous financial statements. Therefore, it is highly\nunlikely that the model is inadvertently using its “memory” about financial information to\nmake earnings predictions.\nAnalysis outside of GPT’s training window As suggested in Sarkar and Vafa (2024),\nthe most effective way to rule out the model’s look-ahead bias is to perform a test outside\n23Compared to de-identified financial statement data, anonymizing textual data is conceptually more\nchallenging. Textual data, such as earnings calls, may still retain sufficient contextual information that\npotentially allows the model to guess the anonymized firm.\n24Refer to Appendix D for computing the accuracy of a random guess.\nFinancial Statement Analysis with Large Language Models 24\nof the model’s training window. OpenAI’s GPT4-Turbo preview was trained on data up to\nApril 2023, thereby significantly limiting the scope to conduct this analysis. Nevertheless,\nwe use financial statement data from fiscal year 2022 (released in January-March 2023) to\npredict earnings of fiscal year 2023 (released in early 2024).\nWe present the results in Table 6, Panel C. As a comparison, we also report prediction\nresults of the logistic regressions, analyst predictions, and ANN models. GPT achieves an\naccuracy of 58.96% and an F1 score of 63.91%. The accuracy (but not the F1 score) is slightly\nlower than the average reported in Table 4, Panel A. However, recall that we find an overall\ndecreasing time trend in GPT’s prediction accuracy. Specifically, as shown in Appendix\nA, GPT’s prediction accuracy is only 54.36% for the fiscal year 2021, and 59.01% for 2019\n(GPT’s prediction accuracy plummets in 2020 during Covid-19 outbreak). In fact, both\nthe out-of-GPT-sample accuracy and the F1 score are substantially higher than the average\nover the last 10 years (58.01% and 59.15%). Therefore, we interpret our results as GPT’s\nout-of-sample performance being closely in line with our “in-sample” results. Furthermore,\nGPT achieves a very similar accuracy score out-of-sample as the ANN model (58.96% versus\n59.10%) and an even higher F1 score (63.91% versus 61.13%) for the same year, which is\nclosely in line with our main findings. Taken together, this result corroborates our prior tests\nand confirms that the model’s predictive ability does not stem from its training memory.\n6.2 Are LLM-Generated Texts Informative?\nNext, we explore whether the model’s predictive ability comes from its ability to generate\nnarrative insights about the financial health and future performance of the company, in\nline with its objective to analyze financial statements. We leverage the fact that our CoT\nprompts instructed the model to provide information besides the prediction itself: narrative\ndescription and interpretations of trend and ratio analyses, as well as the rationale behind the\nbinary predictions. We start with descriptive analyses of the generated texts. Subsequently,\nwe evaluate the information content of texts generated by GPT.\nDescriptive Bigram Analysis We begin with a descriptive approach, performing a con-\ntent analysis of the texts generated by the model. This analysis involves counting the most\ncommon bigrams in the ratio analysis and the most common monograms (single words) in\nthe rationale section. This method allows us to discern patterns and dominant themes that\nmay contribute to the model’s analytical performance.\nWe present the results in Figure 7. In the left panel, we report the top ten most fre-\nquently used bigrams in the ratio analysis. We calculate the frequency by scaling the bigram\ncounts with the total number of bigrams generated by the model. We find that the model\nFinancial Statement Analysis with Large Language Models 25\nmost commonly refers to the operating margin. In addition to the profitability information,\nthe model also frequently computes efficiency (asset and inventory turnover) and liquidity\n(current ratio, current assets, and current liability). The model’s rationale in making fi-\nnal predictions is generally consistent with its bigram analysis. In its decision, the model\ncommonly refers to firm growth, liquidity, operating profitability, and efficiency. This con-\nsistent alignment between the themes identified in the bigram analysis and the model’s final\npredictions underscores the utility of LLM-generated texts in capturing essential financial\nindicators.\nInformation Content of Generated Text We hypothesize that GPT is capable of\npredicting future earnings because it distills narrative insights about the financial health\nof the company from the numeric data. We thus examine whether GPT-generated texts\ncontain information that is useful for predicting the direction of future earnings. To do so,\nwe process each GPT output with a BERT-base-uncased model to obtain its 768-dimensional\nvector representation (note that GPT does not allow retrieving native embeddings, and thus,\nwe use BERT).25We then design a new ANN model that uses these textual embeddings as\ninputs and train the ANN to predict the direction of future earnings (target variable). The\nmodel has two hidden layers, with dimensions of 256 and 64, and an output layer with two\ndimensions: probabilities of earnings increase vs. decrease ( p1,p2). We classify the outcome\nas an increase when p1>p 2and vice versa. The model is otherwise analogous to the ANN\nmodels we estimated earlier.26We refer to this model as the embeddings-based model.\nWe report the accuracy, F1-score, and the area under the ROC curve (AUC) of the\ntrained model in Table 7, Panel B (note that we were not able to measure AUC for GPT\nforecasts and thus did not report it previously). Our embedding model achieves an accuracy\nof 58.95%, an F1-score of 65.26%, and an AUC of 64.22%. It is noteworthy that this model\nachieves the highest F1 score among all classification methods we examined previously. For\ncomparison purposes, the second row of the table repeats the results of the ANN model based\non variables from the two financial statements, which was previously reported in Table 4.\nThis model achieves only a somewhat higher accuracy of 60.12%, but a considerably lower\nF1-score (61.30%) and AUC (59.13%). Overall, our results indicate that narrative text\n25We use the last hidden stage vector of the CLS token associated with a given narrative. In case the\nnarrative exceeds 512 tokens, we partition the text into chunks and take the average over chunk-specific\nvectors.\n26ReLU activation function is used for the first two layers and the sigmoid function is used in the last\nlayer. We minimize cross-entropy loss and use the Adam optimizer. As in our main ANN model, we use\nrolling five-year windows to train the model. We use a batch size of 128 and the model stops training when\nthere is no improvement for five consecutive training epochs. We perform a grid search of nine iterations,\nusing three values of learning rates (1 e−5,1e−3,and 1 e−1) and three values of dropout rates (0, 0.2, and\n0.4), on random 20% of the training sample.\nFinancial Statement Analysis with Large Language Models 26\ngenerated by GPT contains a significant amount of information useful in predicting future\nearnings, i.e., it indeed represents narrative insights derived from numeric data based on the\nCoT prompt. This result suggests that the narrative insights serve as the basis for GPT’s\nsuperior predictive ability. In untabulated results, we find that the correlation between GPT\nforecasts and the embeddings-based forecasts of future earnings direction have a correlation\nof 94%, which suggests that both rely largely on the same information set.\nAs additional analyses, we experiment with different ANN specifications by changing the\ninput vectors. First, following Kim and Nikolaev (2023a), we include both textual vectors\n(GPT insights) and numeric data (scaled variables from financial statements) into the model,\nallowing for full non-linear interactions among the two inputs. This model is reported in row\n(3) of the table. We find that the dual-input model achieves the highest accuracy metrics:\naccuracy of 63.16%, an F1-score of 66.33%, and an AUC of 65.90%. This result reconciles\nwith our prior evidence that GPT forecasts have incremental information beyond numeric\ninputs and also highlights the value of considering the narrative insights generated by an\nLLM when interpreting numerical information.\nFinally, we examine the relative importance of different parts of the financial statement\nanalysis performed by GPT. Specifically, the model analyzes trends, then switches to the\nratio analysis, and concludes by providing a rationale behind its prediction. We obtain em-\nbedding vectors for each of the three types of generated narratives with the goal of assessing\ntheir relative importance. Specifically, we estimate three ANN models each of which leaves\nout one type of embedding vectors from the analysis. The ANN model that omits trend anal-\nysis exhibits an accuracy of 57.11%, which is approximately 1.8 percentage points lower than\nthat of the ANN model that uses the entire text embedding. The ANN model, excluding\nratio analysis, achieves an accuracy of 55.65%, which is almost 3.3 percentage points lower\nthan that of the full ANN model. These results indicate that ratio and, subsequently, trend\nanalysis add the highest and second highest informational value, respectively, when deter-\nmining the future direction of the company. In contrast, excluding the rationale narrative\ndoes not change the model performance substantially (58.88%), implying that the rationale\ndoes not add information beyond the trend and ratio analyses.\n7 Asset Pricing Tests\nHaving demonstrated that GPT’s predictions of the earnings direction have high accu-\nracy and stem from the model’s ability to generate insights rather from memory, we now\ninvestigate the practical value of an LLM-based financial statement analysis by evaluating\ntrading strategies based on GPT’s output.\nFinancial Statement Analysis with Large Language Models 27\nIn particular, signals that are informative about future expected profits should exhibit\na positive association with expected stock returns in the cross-section of firms (Fama and\nFrench, 2015). The asset pricing models typically use the current level of profitability as a\nproxy for future expected future profitability (Novy-Marx, 2013). To the extent GPT fore-\ncasts have incremental information about future profitability, they should also predict future\nstock returns. We use GPT forecasts of whether earnings are likely to increase or decrease\nin the subsequent period, to form an investment strategy and evaluate its performance.\n7.1 Methodology\nBecause our sample includes firms with December 31 fiscal year-end, their financial results\nare released by the end of March. Following prior literature, we allow approximately three\nmonths for the market to fully process the reported information and form portfolios on June\n30 of each year. We hold the portfolio for one year and measure their Sharpe ratios and\nmonthly alphas. We compare three types of strategies. The first strategy sorts stocks into\nportfolios based on GPT forecasts, and the other two perform sorts based on ANN and\nlogistic regression forecasts that rely on numeric information.\nANN and Logistic Regressions ANN and logistic regressions yield probabilities that\nearnings will increase in the subsequent year. We use these predicted probabilities to sort\nthe stocks into ten portfolios. Then on June 30, each year, we take long positions in the top\ndecile stocks and short stocks in the bottom decile.\nGPT Because GPT does not provide probabilities that earnings will increase or decrease,\nwe follow a different approach to form portfolios. We rely on three pieces of information:\nbinary directional prediction, magnitude prediction, and average log probability of tokens.\nIn particular, for each fiscal year, we select stocks predicted to experience an “increase”\nin earnings with the predicted magnitude (of the change in earnings) of either “moderate”\nor ”large. ” Then we sort those stocks on the average log probability values associated with\nthe generated text. This allows us to choose stocks with relatively more confident forecasts\n(recall that model answers with high certainty are more accurate than the ones with low\ncertainty). We then retain stocks with the highest log probabilities such that the number of\nfirms retained each year constitutes 10% of our sample in that year (our goal is to construct\nan equivalent to a decile portfolio). We also do the same for the stocks predicted to experience\na “decrease” in earnings. We filter stocks with a predicted magnitude of either “moderate”\nor “large”, and sort them on log probability values. We then short the same number of stocks\nas that in the long portfolio, i.e., retain 10% from the total number of observations in that\nFinancial Statement Analysis with Large Language Models 28\nyear with the highest expected confidence. By doing so, we match the number of stocks to\nthe number of stocks included in ANN or logit-based portfolios.\n7.2 Results\nSharpe Ratios To compute Sharpe ratios, we form equal-weighted and value-weighted\nportfolios. For value-weighted portfolios, we rebalance the portfolio weights each month.\nAlthough value-weighted portfolios are less sensitive to small market capitalizations, it is\ndifficult to rebalance the portfolios based on the stocks’ time-varying market caps in practice\n(Jiang et al., 2022). Recall that our prior findings suggest that GPT appears to have an\nadvantage in analyzing smaller and relatively more volatile companies. We thus present the\noutcome of both the value- and equal-weighted strategies.\nThe results are presented in Table 8, Panel A. We find that equal-weighted portfolios\nbased on GPT predictions achieve a Sharpe ratio of 3.36, which is substantially larger than\nthe Sharpe ratio of ANN-based portfolios (2.54) or logistic regression-based portfolios (2.05).\nIn contrast, for value-weighted portfolios, we observe that ANN performs relatively better\n(Sharpe = 1.79) than GPT (1.47). Both dominate the logistic regressions (0.81).27This\nresult is consistent with our finding in Table 4 that both GPT and ANN contain incremental\ninformation and are thus complementary. Overall, this analysis shows potential for using\nGPT-based financial statement analysis to derive profitable trading strategies.\nAlphas Next, we compute monthly alphas for each of the three investment strategies\ndescribed above based on five different factor models, from CAPM to Fama and French\n(2015)’s five factors plus momentum. We present the results in Table 8, Panel B.\nConsistent with the results in Panel A, equal-weighted portfolios generate higher alphas\nin general. As expected, we observe a significant reduction in alphas when we include the\nprofitability factor in column (4) (from 1.29 to 0.97 for portfolios based on GPT predictions),\nwhich is another proxy for future profitability. However, even after controlling for five factors\nand momentum, portfolios based on GPT’s predictions generate a monthly alpha of 84 basis\npoints (column (5)), or 10% annually. Portfolios based on ANN and logistic regression\nestimates also generate positive alphas. However, their magnitudes and economic significance\nare smaller (60 basis points with a t-statistic of 1.89 for ANN and 43 basis points with a\nt-statistic of 1.96 for logistic regressions).\nIn Figure 8, we plot the cumulative log returns of portfolios based on GPT’s predictions\nfrom 1968 to 2021. The left panel shows the cumulative log returns for equal-weighted long\n27Accounting for 10 basis points in transaction costs, the GPT-based equal-weighted portfolio yields a\nSharpe ratio of 2.84. The value-weighted portfolio yields a Sharpe ratio of 0.95.\nFinancial Statement Analysis with Large Language Models 29\nand short portfolios separately. As expected, the long portfolio substantially outperforms\nthe short portfolio. In the right panel, we plot the cumulative log returns for the long-short\nportfolio and compare them with the log market portfolio returns (dotted line). Notably,\nour long-short portfolio consistently outperforms the market portfolio even when the market\nexperiences negative cumulative returns.\nFor value-weighted portfolios, consistent with Sharpe ratio results, ANN-based portfolios\nperform better compared to GPT with 50 basis points alphas even after controlling for the\nfive factors and momentum. Portfolios based on GPT’s predictions achieve 37 basis points\nalpha with a t-statistic of 2.43 (column (10)). Portfolios based on logit estimates also exhibit\npositive alphas (31 basis points) though they are marginally insignificant ( t-statistic = 1.55).\nOverall, our analysis demonstrates the value of GPT-based fundamental analysis in stock\nmarkets. We also note that the stronger (weaker) GPT’s performance compared to ANN\nwhen evaluated on equal-weighted (value-weighed) strategies is intriguing and points to\nGPT’s ability to uncover value in smaller stocks.\n8 Conclusion\nIn this paper, we probe the limits of large language models by providing novel evidence\non their ability to analyze financial statements. Financial statement analysis is a traditional\nquantitative task that requires, critical thinking, reasoning, and judgment. Our approach\ninvolves providing the model with structured and anonymized financial statements and a\nsophisticated chain-of-thought prompt that mimics how human analysts process financial\ninformation. We specifically do not provide any narrative information.\nOur results suggest that GPT’s analysis yields useful insights about the company, which\nenable the model to outperform professional human analysts in predicting the direction of\nfuture earnings. We also document that GPT and human analysts are complementary, rather\nthan substitutes. Specifically, language models have a larger advantage over human analysts\nwhen analysts are expected to exhibit bias and disagreement, suggesting that AI models\ncan assist humans better when they are under-performing. Humans, on the other hand add\nvalue when additional context, not available to the model is likely to be important.\nFurthermore and surprisingly, GPT’s performance is on par (or even better in some cases)\nwith that of the most sophisticated narrowly specialized machine learning models, namely,\nan ANN trained on earnings prediction tasks. We investigate potential sources of the LLM’s\nsuperior predictive power. We first rule out that the model’s performance stems from its\nmemory. Instead, our analysis suggests that the model draws its inference by gleaning use-\nful insights from its analysis of trends and financial ratios and by leveraging its theoretical\nFinancial Statement Analysis with Large Language Models 30\nunderstanding and economic reasoning. Notably, the narrative financial statement analy-\nsis generated by the language model has substantial informational value in its own right.\nBuilding on these findings, we also present a profitable trading strategy based on GPT’s\npredictions. The strategy yields higher Sharpe ratios and alphas than other trading strate-\ngies based on ML models. Overall, our analysis suggests that GPT shows a remarkable\naptitude for financial statement analysis and achieves state-of-the-art performance without\nany specialized training.\nAlthough one must interpret our results with caution, we provide evidence consistent\nwith large language models having human-like capabilities in the financial domain. General-\npurpose language models successfully perform a task that typically requires human expertise\nand judgment and do so based on data exclusively from the numeric domain. Therefore, our\nfindings indicate the potential for LLMs to democratize financial information processing and\nshould be of interest to investors and regulators. For example, our results suggest that\ngenerative AI is not merely a tool that can assist investors (e.g., in summarizing financial\nstatements, Kim et al., 2023b), but can play a more active role in making informed decisions.\nThis finding is significant, as unsophisticated investors might be prone to ignoring relevant\nsignals (e.g., Blankespoor et al., 2019), even if they are generated by advanced AI tools.\nHowever, whether AI can substantially improve human decision-making in financial markets\nin practice is still to be seen. We leave this question for future research. Finally, even though\nwe strive to understand the sources of model predictions, it is empirically difficult to pinpoint\nhowandwhythe model performs well.\nFinancial Statement Analysis with Large Language Models 31\nReferences\nAbarbanell, J.S., Bernard, V.L., 1992. Tests of analysts’ overreaction/underreaction to earnings\ninformation as an explanation for anomalous stock price behavior. The Journal of Finance 47,\n1181–1207.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al., 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\nAshbaugh-Skaife, H., Collins, D.W., Kinney Jr, W.R., 2007. The discovery and reporting of internal\ncontrol deficiencies prior to sox-mandated audits. Journal of Accounting and Economics 44, 166–\n192.\nBall, R., Foster, G., 1982. Corporate financial reporting: A methodological review of empirical\nresearch. Journal of Accounting Research , 161–234.\nBall, R., Kothari, S., Robin, A., 2000. The effect of international institutional factors on properties\nof accounting earnings. Journal of Accounting and Economics 29, 1–51.\nBall, R., Sadka, G., Tseng, A., 2022. Using accounting earnings and aggregate economic indicators\nto estimate firm-level systematic risk. Review of Accounting Studies 27, 607–646.\nBasu, S., Markov, S., 2004. Loss function assumptions in rational expectations tests on financial\nanalysts’ earnings forecasts. Journal of Accounting and Economics 38, 171–203.\nBernard, D., Blankespoor, E., de Kok, T., Toynbee, S., 2023. Confused readers: A modular measure\nof business complexity. University of Washington Working Paper.\nBlankespoor, E., Dehaan, E., Wertz, J., Zhu, C., 2019. Why do individual investors disregard\naccounting information? the roles of information awareness and acquisition costs. Journal of\nAccounting Research 57, 53–84.\nBochkay, K., Levine, C.B., 2019. Using md&a to improve earnings forecasts. Journal of Accounting,\nAuditing & Finance 34, 458–482.\nBordalo, P., Gennaioli, N., Porta, R., Shleifer, A., 2019. Diagnostic expectations and stock returns.\nThe Journal of Finance 74, 2839–2874.\nBouwman, M.J., Frishkoff, P.A., Frishkoff, P., 1987. How do financial analysts make decisions? a\nprocess model of the investment screening decision. Accounting, Organizations and Society 12,\n1–29.\nBradshaw, M.T., 2011. Analysts’ forecasts: what do we know after decades of work? Working\nPaper .\nBradshaw, M.T., Drake, M.S., Myers, J.N., Myers, L.A., 2012. A re-examination of analysts’\nsuperiority over time-series forecasts of annual earnings. Review of Accounting Studies 17, 944–\n968.\nBrown, L.D., Call, A.C., Clement, M.B., Sharp, N.Y., 2015. Inside the “black box” of sell-side\nfinancial analysts. Journal of Accounting Research 53, 1–47.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al., 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33, 1877–1901.\nBybee, J.L., 2023. The ghost in the machine: Generating beliefs with large language models. arXiv\npreprint arXiv:2305.02823 .\nCao, S., Jiang, W., Wang, J.L., Yang, B., 2024. From man vs. machine to man+ machine: The art\nand ai of stock analyses. Journal of Financial Economics Forthcoming.\nChen, X., Cho, Y.H., Dou, Y., Lev, B., 2022. Predicting future earnings changes using machine\nFinancial Statement Analysis with Large Language Models 32\nlearning and detailed financial data. Journal of Accounting Research 60, 467–515.\nChoi, G.Y., Kim, A., 2023. Economic footprints of tax audits: A generative ai-driven approach.\nChicago Booth Research Paper .\nChoi, J.H., Hickman, K.E., Monahan, A.B., Schwarcz, D., 2022. Chatgpt goes to law school. J.\nLegal Educ. 71, 387.\nCostello, A.M., Down, A.K., Mehta, M.N., 2020. Machine+ man: A field experiment on the role\nof discretion in augmenting ai-based lending models. Journal of Accounting and Economics 70,\n101360.\nDe Bondt, W.F., Thaler, R.H., 1990. Do security analysts overreact? The American Economic\nReview , 52–57.\nDell’Acqua, F., McFowland, E., Mollick, E.R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S.,\nKrayer, L., Candelon, F., Lakhani, K.R., 2023. Navigating the jagged technological frontier:\nField experimental evidence of the effects of ai on knowledge worker productivity and quality.\nHBS Working Paper .\nDonelson, D.C., Resutek, R.J., 2015. The predictive qualities of earnings volatility and earnings\nuncertainty. Review of Accounting Studies 20, 470–500.\nEulerich, M., Sanatizadeh, A., Vakilzadeh, H., Wood, D.A., 2023. Can artificial intelligence pass\naccounting certification exams? chatgpt: Cpa, cma, cia, and ea? ChatGPT: CPA, CMA, CIA,\nand EA .\nFama, E.F., French, K.R., 2015. A five-factor asset pricing model. Journal of Financial Economics\n116, 1–22.\nGe, W., McVay, S., 2005. The disclosure of material weaknesses in internal control after the\nsarbanes-oxley act. Accounting Horizons 19, 137–158.\nGlasserman, P., Lin, C., 2023. Assessing look-ahead bias in stock return predictions generated by\ngpt sentiment analysis. arXiv preprint arXiv:2309.17322 .\nGraham, B., Dodd, D.L., 1934. Security Analysis. Whittlesey House, McGraw-Hill Book Co., New\nYork.\nGreen, J., Hand, J.R., Zhang, X.F., 2016. Errors and questionable judgments in analysts’ dcf\nmodels. Review of Accounting Studies 21, 596–632.\nGroysberg, B., Healy, P.M., Maber, D.A., 2011. What drives sell-side analyst compensation at\nhigh-status investment banks? Journal of Accounting Research 49, 969–1000.\nHansen, A.L., Kazinnik, S., 2023. Can chatgpt decipher fedspeak? Available at SSRN .\nHunt, J.O., Myers, J.N., Myers, L.A., 2022. Improving earnings predictions and abnormal returns\nwith machine learning. Accounting Horizons 36, 131–149.\nHutton, A.P., Lee, L.F., Shu, S.Z., 2012. Do managers always know better? the relative accuracy\nof management and analyst forecasts. Journal of Accounting Research 50, 1217–1244.\nHwang, L.S., Jan, C.L., Basu, S., 1996. Loss firms and analysts’ earnings forecast errors. The\nJournal of Financial Statement Analysis 1.\nJiang, J., Kelly, B.T., Xiu, D., 2022. Expected returns and large language models. Available at\nSSRN .\nKahneman, D., 2011. Thinking, fast and slow. macmillan.\nKim, A., Muhn, M., Nikolaev, V., 2023a. From transcripts to insights: Uncovering corporate risks\nusing generative ai. arXiv preprint arXiv:2310.17721 .\nKim, A., Muhn, M., Nikolaev, V.V., 2023b. Bloated disclosures: can chatgpt help investors process\ninformation? Chicago Booth Research Paper .\nKim, A., Nikolaev, V.V., 2023a. Context-based interpretation of financial information. Chicago\nFinancial Statement Analysis with Large Language Models 33\nBooth Research Paper .\nKim, A.G., Nikolaev, V.V., 2023b. Profitability context and the cross-section of stock returns.\nChicago Booth Research Paper .\nKothari, S., So, E., Verdi, R., 2016. Analysts’ forecasts and asset pricing: A survey. Annual Review\nof Financial Economics 8, 197–219.\nLi, E.X., Tu, Z., Zhou, D., 2023. The promise and peril of generative ai: Evidence from chatgpt as\nsell-side analysts. Available at SSRN 4480947 .\nLiu, M., 2022. Assessing human information processing in lending decisions: A machine learning\napproach. Journal of Accounting Research 60, 607–651.\nLopez-Lira, A., Tang, Y., 2023. Can chatgpt forecast stock price movements? return predictability\nand large language models. arXiv preprint arXiv:2304.07619 .\nMonahan, S.J., et al., 2018. Financial statement analysis and earnings forecasting. Foundations\nand Trends ®in Accounting 12, 105–215.\nNovy-Marx, R., 2013. The other side of value: The gross profitability premium. Journal of financial\neconomics 108, 1–28.\nOu, J.A., Penman, S.H., 1989. Financial statement analysis and the prediction of stock returns.\nJournal of Accounting and Economics 11, 295–329.\nPenman, S.H., 2001. On comparing cash flow and accrual accounting models for use in equity\nvaluation: A response to lundholm and o’keefe (car, summer 2001). Contemporary Accounting\nResearch 18, 681–692.\nPenman, S.H., Sougiannis, T., 1998. A comparison of dividend, cash flow, and earnings approaches\nto equity valuation. Contemporary accounting research 15, 343–383.\nPiotroski, J.D., 2000. Value investing: The use of historical financial statement information to\nseparate winners from losers. Journal of Accounting Research , 1–41.\nSarkar, S.K., Vafa, K., 2024. Lookahead bias in pretrained language models. Available at SSRN .\nSloan, R.G., 2019. Fundamental analysis redux. The Accounting Review 94, 363–377.\nStickel, S.E., 1991. Common stock returns surrounding earnings forecast revisions: More puzzling\nevidence. The Accounting Review , 402–416.\nWatts, R.L., Zimmerman, J.L., 1986. Positive accounting theory .\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural\nInformation Processing Systems 35, 24824–24837.\nFinancial Statement Analysis with Large Language Models 34\nAppendix A. Time Series of GPT’s Prediction Accuracy\nThis table shows time-series prediction accuracy and F1 scores of GPT and ANN. The last two columns are the differences\nbetween the two models (GPT - ANN). Time trend is obtained by regressing accuracy metrics on fiscal years, obtaining robust\nstandard errors at the year level. *, **, and *** denote statistical significance at 10%, 5%, and 1% levels, respectively.\nGPT ANN Diff\nFiscal Year Accuracy F1 Accuracy F1 Accuracy F1\n1968 58.55% 67.19% 58.48% 67.45% 0.07% -0.26%\n1969 59.23% 59.85% 58.71% 59.32% 0.52% 0.53%\n1970 55.51% 58.86% 55.27% 58.66% 0.24% 0.20%\n1971 60.29% 70.33% 59.73% 69.89% 0.56% 0.44%\n1972 72.96% 81.58% 71.26% 80.62% 1.70% 0.96%\n1973 67.53% 74.97% 66.84% 74.60% 0.69% 0.37%\n1974 57.32% 63.10% 55.93% 61.93% 1.39% 1.17%\n1975 58.29% 67.21% 57.93% 66.89% 0.36% 0.32%\n1976 68.31% 77.63% 68.00% 77.42% 0.31% 0.21%\n1977 69.14% 78.30% 68.64% 77.96% 0.50% 0.34%\n1978 69.84% 78.73% 69.26% 78.27% 0.58% 0.46%\n1979 61.90% 67.70% 60.96% 66.84% 0.94% 0.86%\n1980 61.98% 64.72% 61.04% 63.97% 0.94% 0.75%\n1981 61.00% 58.45% 59.78% 57.37% 1.22% 1.08%\n1982 58.89% 58.04% 57.69% 56.75% 1.20% 1.29%\n1983 66.32% 71.88% 64.96% 70.93% 1.36% 0.95%\n1984 57.82% 59.54% 56.45% 58.14% 1.37% 1.40%\n1985 58.19% 53.48% 56.44% 51.33% 1.75% 2.15%\n1986 60.45% 60.82% 58.92% 59.48% 1.53% 1.34%\n1987 63.85% 69.07% 62.45% 67.90% 1.40% 1.17%\n1988 61.01% 63.50% 59.84% 62.32% 1.17% 1.18%\n1989 60.36% 59.25% 59.39% 58.30% 0.97% 0.95%\n1990 59.16% 57.50% 58.78% 57.08% 0.38% 0.42%\n1991 60.04% 58.91% 59.63% 58.29% 0.41% 0.62%\n1992 58.64% 61.49% 58.09% 60.75% 0.55% 0.74%\n1993 66.79% 70.03% 62.25% 66.47% 4.54% 3.56%\n1994 64.58% 69.70% 63.48% 68.69% 1.10% 1.01%\n1995 60.84% 65.53% 59.87% 64.13% 0.97% 1.40%\n1996 63.96% 67.09% 63.00% 65.88% 0.96% 1.21%\n1997 56.38% 56.57% 55.40% 55.04% 0.98% 1.53%\n1998 56.24% 58.79% 54.46% 56.38% 1.78% 2.41%\n1999 62.39% 64.27% 61.08% 62.82% 1.31% 1.45%\n2000 55.27% 51.77% 54.22% 50.66% 1.05% 1.11%\n2001 56.65% 54.92% 56.02% 54.38% 0.63% 0.54%\n2002 56.51% 62.80% 55.68% 62.24% 0.83% 0.56%\n2003 59.94% 66.97% 59.92% 67.22% 0.02% -0.25%\n2004 60.59% 67.68% 60.13% 67.54% 0.46% 0.14%\n2005 60.17% 65.35% 59.69% 64.76% 0.48% 0.59%\n2006 61.36% 63.24% 60.69% 62.46% 0.67% 0.78%\n2007 60.73% 52.21% 60.27% 52.32% 0.46% -0.11%\n2008 51.06% 43.49% 50.62% 43.33% 0.44% 0.16%\n2009 48.86% 51.61% 48.57% 51.76% 0.29% -0.15%\n2010 59.28% 66.29% 58.68% 65.98% 0.60% 0.31%\n2011 57.52% 62.11% 57.36% 62.02% 0.16% 0.09%\n2012 60.09% 63.62% 59.36% 62.49% 0.73% 1.13%\n2013 60.81% 63.28% 60.21% 62.49% 0.60% 0.79%\n2014 59.54% 59.75% 59.03% 59.04% 0.51% 0.71%\n2015 59.76% 60.33% 59.18% 59.09% 0.58% 1.24%\n2016 58.49% 59.44% 57.91% 58.77% 0.58% 0.67%\n2017 59.35% 64.70% 58.41% 64.15% 0.94% 0.55%\n2018 58.40% 60.31% 57.75% 59.52% 0.65% 0.79%\n2019 59.01% 49.73% 58.35% 49.50% 0.66% 0.23%\n2020 50.25% 55.24% 49.72% 54.41% 0.53% 0.83%\n2021 54.36% 55.13% 53.54% 59.84% 0.82% -4.71%\nTrend -0.001*** -0.002*** -0.001*** -0.002*** -0.000 -0.000\n(-3.17) (-3.95) (-3.13) (-3.71) (-1.49) (-1.12)\nFinancial Statement Analysis with Large Language Models 35\nAppendix B. Example Balance Sheet and Income Statement\nPanel A and Panel B show an example of standardized, anonymous balance sheet and income statement. We use Compustat’s\nbalancing formula and delete fiscal years.\nPanel A. Balance Sheet\nPanel B. Income Statement\n\nFinancial Statement Analysis with Large Language Models 36\nAppendix C. Example Output\nWe present one example output by GPT. GPT has rendered a prediction of “increase” with a moderate magnitude, and a\nprediction certainty of 0.7. The correct prediction is “increase.” Panel A shows the trend analysis results, Panel B shows ratio\nanalysis results, and Panel C shows the rationale.\nPanel A. Trend Analysis\nPanel B. Ratio Analysis\nPanel C. Rationale\n\nFinancial Statement Analysis with Large Language Models 37\nAppendix D. GPT’s Guess About Fiscal Years\nIn Table 6, we show that the accuracy of GPT’s fiscal year guesses is 2.95%. Our sample\nspans the period 1968-2021, and one might have concern that a pure random guess leads to\na probability of 1.85%, which is lower than GPT’s accuracy. However, given the distribution\nin GPT’s answers and the distribution in our universe, this is not the case.\nWe observe that out of 10,000 random samples, GPT’s fiscal year predictions only\ngive years 2001 (0.02%), 2008 (0.47%), 2018 (0.02%), 2019 (3.50%), 2020 (32.60%), 2021\n(63.31%), and 2023 (0.09%). GPT’s inability to produce balanced guesses already suggests\nthat it cannot make informed guesses about fiscal years. However, to test this more formally,\nassume that we randomly draw a sample from the universe and that its fiscal year is i. When\niis not included in 2001, 2008, 2018, 2019, 2020, 2021, and 2023, the probability that GPT\nwill guess the correct year is zero. If iis 2001, the probability that GPT will guess the\ncorrect year is 0.02% and if iis 2021, the probability increases to 63.31%. Now define pias\nthe probability that GPT will guess the correct fiscal year given year i.\nOne more thing that we should consider is that our universe is not a balanced panel. Our\ndata is sparse in earlier years and more dense in recent years. Fiscal year 2021, for instance,\naccount for 3.5% of the total observations. Let qibe the proportion of fiscal year iin the\nentire sample. Then, the expected probability that a random draw from the population leads\nto a correct guess by GPT is\nProb =2021/summationdisplay\ni=1968piqi\nThis value is 3.3%, which is higher than 2.95%, the value we report in Table 6.\nFinancial Statement Analysis with Large Language Models 38\nFigure 1 .GPT Processing Details\nThis figure illustrates the structure of our experiment. Using raw data from Compustat annual, we construct standardized\nbalance sheet and income statement using Compustat’s balancing formulae. Then, we substitute fiscal years with relative fiscal\nyears t,t−1, and t−2. We then provide these anonymous, standardized financial statements to GPT 4 Turbo with detailed\nchain-of-thought prompts. The model is instructed to provide notable trends, financial ratios, and their interpretations. Final\npredictions are binary (increase or decrease) along with a paragraph of rationale. We also instruct the model to produce the\npredicted magnitude of earnings change and the confidence of its answers.\n\nFinancial Statement Analysis with Large Language Models 39\nFigure 2 .GPT vs. Human Analysts\nThis figure compares the prediction performance of GPT and human analysts. Random Walk is based on the current earnings\nchange compared to the previous earnings. Analyst 1m (3m, 6m) denotes the median analyst forecast issued one (three, six)\nmonth(s) after the earnings release. GPT (wihtout CoT) denotes GPT’s predictions without any chain-of-thought prompts.\nWe simply provide the model with structured and anonymous financial statement information. GPT (with CoT) denotes the\nmodel with financial statement information and detailed chain-of-thought prompts. We report average accuracy (the percentage\nof correct predictions out of total predictions) for each method (left) and F1 score (right). We obtain bootstrapped standard\nerrors by randomly sampling 1,000 observations 1,000 times and include 95% confidence intervals.\n\nFinancial Statement Analysis with Large Language Models 40\nFigure 3 .GPT vs. Machine Learning Models\nThis figure compares the prediction performance of GPT and quantitative models based on machine learning. Stepwise Logistic\nfollows Ou and Penman (1989)’s structure with their 59 financial predictors. ANN is a three-layer artificial neural network model\nusing the same set of variables as in Ou and Penman (1989). GPT (with CoT) provides the model with financial statement\ninformation and detailed chain-of-thought prompts. We report average accuracy (the percentage of correct predictions out of\ntotal predictions) for each method (left) and F1 score (right). We obtain bootstrapped standard errors by randomly sampling\n1,000 observations 1,000 times and include 95% confidence intervals.\n\nFinancial Statement Analysis with Large Language Models 41\nFigure 4 .Time Trend in Prediction Accuracy\nThis figure illustrates the time trend in GPT’s prediction accuracy (left) and the difference in GPT and ANN’s prediction\naccuracy (right). Left panel demonstrates annual accuracy of GPT’s predictions. The dotted line represents fitted time-trend.\nIn right panel, we compute the difference between GPT’s and ANN’s prediction accuracy each year (GPT’s accuracy - ANN’s\naccuracy). The dotted line represents fitted time-trend.\n\nFinancial Statement Analysis with Large Language Models 42\nFigure 5 .Different GPT Specifications\nThis figure compares the model performance depending on several experimental settings. The first four bars are based on GPT’s\nanswers on its confidence and the averaged token-level log probabilities. The fifth and six bars are the predicted magnitude in\nearnings change. The last two columns compare the prediction accuracy of GPT 4 and GPT 3.5. We use a random 20% sample\nfor the last two columns. We obtain bootstrapped standard errors by randomly sampling 1,000 observations 1,000 times and\ninclude 95% confidence intervals.\n\nFinancial Statement Analysis with Large Language Models 43\nFigure 6 .GPT’s Memory\nThis figure shows the experiment results to test GPT’s memory. We ask GPT to produce ten most probable company names\nand the most probable fiscal year from the standardized, anonymous financial statements. Left panel shows the ten most\nfrequent company names in GPT’s answers, and right panel plots the actual fiscal years (vertical axis) and predicted fiscal years\n(horizontal axis).\n\nFinancial Statement Analysis with Large Language Models 44\nFigure 7 .Sources of Prediction\nThis figure shows descriptive bigram (monogram) frequency counts of GPT answers. Left panel shows the ten most frequently\nused bigrams in GPT’s answers on the financial ratio analysis. Right panel shows the ten most frequently used monograms in\nGPT’s answers on the rationale behind its binary earnings prediction.\n\nFinancial Statement Analysis with Large Language Models 45\nFigure 8 .Equal-Weight Portfolio Cumulative Returns Over Time\nThis figure shows cumulative log returns from 1968 to 2021 of the long-short strategies based on GPT predictions. We form\nequal-weight portfolios on June 30 of each year and hold them for one year. We long the top decile of stocks that are classified\nas “increase” in earnings prediction, “large” or “moderate” in magnitude, based on their log probabilities. Similarly we short\nthe top decile of stocks that are classified as “decrease” in earnings prediction, “large” or “moderate” in magnitude, based on\ntheir log probabilities. Left panel shows the cumulative log returns of long and short portfolios. Right panel demonstrates\nlong-short returns and the market returns.\n\nFinancial Statement Analysis with Large Language Models 46\nTable 1 .Descriptive Statistics\nThis table shows descriptive statistics of the variables used in analyses. Panel A uses the entire universe of Compustat and\nPanel B uses the intersection between I/B/E/S and Compustat. For Panel B, we require that each observation has at least three\nanalyst forecasts issued. Pred Xdenotes an indicator variable that equals one when method Xpredicts an increase in earnings\nand zero otherwise. Target is an indicator that equals one when earnings increase in the next period and zero otherwise. Sizeis\nthe log of total assets, BtoM is book-to-market ratio, Leverage is total debt over total asset, Earnings Volatility is the standard\ndeviation of earnings over the past five years scaled by total asset, and PP&E is net property, plant, and equipment scaled by\ntotal asset.\nPanel A. Full Sample (1968 – 2021)\nN Mean Std P25 P50 P75\nTarget 150,678 0.555 0.497 0.000 1.000 1.000\nPred GPT 150,678 0.530 0.499 0.000 1.000 1.000\nPred Logit 150,678 0.591 0.493 0.000 1.000 1.000\nPred ANN 150,678 0.521 0.492 0.000 1.000 1.000\nPred Random 150,678 0.561 0.495 0.000 1.000 1.000\nSize 133,830 6.135 2.298 4.441 6.074 7.717\nBtoM 133,830 0.677 0.620 0.289 0.542 0.896\nLeverage 133,830 0.549 0.267 0.348 0.553 0.740\nEarnings Volatility 133,830 0.081 0.177 0.008 0.024 0.071\nPP&E 133,830 0.281 0.270 0.042 0.195 0.454\nPanel B. Analyst Sample (1983 – 2021)\nN Mean Std P25 P50 P75\nTarget 39,533 0.563 0.496 0.000 1.000 1.000\nPred GPT 39,533 0.555 0.497 0.000 1.000 1.000\nPred Analyst1m 39,533 0.518 0.499 0.000 1.000 1.000\nPred Analyst3m 39,533 0.520 0.500 0.000 1.000 1.000\nPred Analyst6m 39,533 0.516 0.500 0.000 1.000 1.000\nPred Random 39,533 0.569 0.495 0.000 1.000 1.000\nSize 37,736 7.541 1.897 6.138 7.504 8.813\nBtoM 37,736 0.528 0.450 0.249 0.444 0.701\nLeverage 37,736 0.580 0.256 0.400 0.582 0.771\nEarnings Volatility 37,736 0.051 0.109 0.007 0.020 0.051\nPP&E 37,736 0.257 0.260 0.038 0.155 0.418\nFinancial Statement Analysis with Large Language Models 47\nTable 2 .GPT vs. Human Analysts\nThis table reports prediction performance of the random walk model, analysts’ forecast issued one month after previous earnings\nrelease (Analyst 1m), three months after previous earnings release (Analyst 3m), and six months after previous earnings release\n(Analyst 6m). GPT (wihtout CoT) denotes GPT’s predictions without any chain-of-thought prompts. We simply provide the\nmodel with structured and anonymous financial statement information. GPT (with CoT) denotes the model with financial\nstatement information and detailed chain-of-thought prompts. Accuracy is the percentage of correct predictions out of total\npredictions. F1 is the harmonic mean of the precision and recall.\nAccuracy F1\nRandom Walk 49.11% 53.02%\nAnalyst 1m 52.71% 54.48%\nAnalyst 3m 55.95% 55.33%\nAnalyst 6m 56.68% 56.85%\nGPT (without CoT) 52.33% 54.52%\nGPT (with CoT) 60.35% 60.90%\nFinancial Statement Analysis with Large Language Models 48\nTable 3 .Complementarities Between Human Analysts and GPT\n*, **, and *** denote statistical significance at 10%, 5%, and 1% levels, respectively.\nIn Panel A, we investigate the determinants of incorrect predictions. I(Incorrect = 1), which is an indicator that equals one\nwhen the model makes incorrect predictions and zero otherwise. Independent variables are defined in Table 1. All continuous\nvariables are winsorized at 1% and 99% level. Standard errors are clustered at the industry level. Column (1) uses GPT for\nI(Incorrect = 1) and columns (2), (3), and (4) use analysts’ predictions. Panel B shows incremental informativeness of each\nprediction. Both independent and dependent variables are indicators. I(Increase = 1) is an indicator that equals one when\nactual earnings increase and zero otherwise. All independent variables are also indicators that equal one when respective method\npredicts an increase in earnings and zero otherwise. Standard errors are clustered at the industry level. In Panel C, we partition\nthe sample based on analyst bias and dispersion. Bias is the forecasted portion of analysts’ forecast error and dispersion is\nthe standard deviation of analyst forecasts scaled by stock price at the end of prior fiscal year. Low and High denote first and\nfourth quartiles, respectively. F-test compares the magnitude of the coefficients on columns (1) and (2), and (3) and (4).\nPanel A. Determinants\nDep Var I(Incorrect=1)\nGPT Analyst 1m Analyst 3m Analyst 6m\n(1) (2) (3) (4)\nSize -0.017*** -0.008*** -0.010*** -0.010***\n(-5.16) (-5.72) (-4.69) (-4.81)\nBtoM -0.022 -0.016*** -0.012** -0.012**\n(-0.99) (-2.94) (-2.21) (-2.35)\nLeverage -0.145 -0.032 -0.029 -0.029\n(-1.50) (-0.37) (-1.40) (-1.36)\nLoss 0.193*** 0.141*** 0.146*** 0.145***\n(4.76) (7.02) (6.90) (6.09)\nEarnings Volatility 0.236*** 0.169*** 0.160*** 0.132**\n(2.69) (4.08) (3.46) (2.47)\nPP&E 0.133* 0.041 0.036* 0.031\n(1.67) (1.18) (1.71) (1.25)\nYear FE Yes Yes Yes Yes\nIndustry FE Yes Yes Yes Yes\nAdjusted R2 0.08 0.027 0.032 0.029\nN 37,736 37,736 37,736 37,736\nPanel B. Incremental Informativeness\nDep Var I(Increase=1)\n(1) (2) (3) (4) (5) (6) (7)\nGPT 0.182*** 0.170*** 0.151** 0.152**\n(2.99) (2.67) (2.35) (2.30)\nAnalyst 1m 0.073*** 0.110**\n(3.11) (2.43)\nAnalyst 3m 0.098*** 0.122***\n(4.02) (3.49)\nAnalyst 6m 0.100*** 0.124***\n(4.05) (3.62)\nYear FE Yes Yes Yes Yes Yes Yes Yes\nIndustry FE Yes Yes Yes Yes Yes Yes Yes\nAdjusted R2 0.07 0.025 0.043 0.044 0.089 0.091 0.091\nN 37,736 37,736 37,736 37,736 37,736 37,736 37,736\nPanel C. Human Bias and Dispersion\nDep Var I(Increase=1)\nBias Dispersion\nLow High Low High\n(1) (2) (3) (4)\nGPT 0.075** 0.341*** 0.118** 0.301***\n(2.21) (4.39) (2.50) (3.20)\nAnalyst 1m 0.175*** 0.093*** 0.187*** 0.058**\n(8.54) (3.05) (6.59) (2.35)\nF-Test on GPT p-value <0.01 p-value <0.01\nF-Test on Analyst p-value <0.01 p-value <0.01\nYear FE Yes Yes Yes Yes\nIndustry FE Yes Yes Yes Yes\nAdjusted R2 0.057 0.134 0.071 0.115\nN 9,410 9,396 9,448 10,093\nFinancial Statement Analysis with Large Language Models 49\nTable 4 .Comparison with ML Benchmarks\n*, **, and *** denote statistical significance at 10%, 5%, and 1% levels, respectively.\nIn Panel A, we compare the prediction performance of GPT and quantitative models based on machine learning. Stepwise\nLogistic follows Ou and Penman (1989)’s structure with their 59 financial predictors. ANN is a three-layer artificial neural\nnetwork model using the same set of variables as in Ou and Penman (1989). GPT (with CoT) provides the model with\nfinancial statement information and detailed chain-of-thought prompts. Accuracy is the percentage of correct predictions out\nof total predictions. F1 is the harmonic mean of the precision and recall. In Panel B, we investigate the determinants of\nincorrect predictions. I(Incorrect = 1), which is an indicator that equals one when the model makes incorrect predictions and\nzero otherwise. Independent variables are defined in Table 1. All continuous variables are winsorized at 1% and 99% level.\nStandard errors are clustered at the industry level. Column (1) uses GPT for I(Incorrect = 1) and columns (2), (3), and (4)\nuse analysts’ predictions. Panel C shows incremental informativeness of each prediction. Both independent and dependent\nvariables are indicators. I(Increase = 1) is an indicator that equals one when actual earnings increase and zero otherwise.\nAll independent variables are also indicators that equal one when respective method predicts an increase in earnings and zero\notherwise. Standard errors are clustered at the industry level.\nPanel A. Other Models\nAccuracy F1\nStepwise Logistic 52.94% 57.23%\nANN (Ou and Penman (1989) variables) 60.45% 61.62%\nANN (Financial statement variables) 60.12% 61.30%\nGPT (with CoT) 60.31% 63.45%\nPanel B. Sources of Inaccuracy\nDep Var = I(Incorrect=1)\nGPT ANN Stepwise Logistic\n(1) (2) (3)\nSize -0.015*** -0.024*** -0.029***\n(-9.09) (-11.33) (-11.56)\nBtoM 0.001 0.002 0.002\n(0.38) (0.73) (0.69)\nLeverage 0.092*** 0.085*** 0.090***\n(6.30) (5.88) (6.02)\nLoss 0.134*** 0.181*** 0.202***\n(9.64) (11.35) (12.96)\nEarnings Volatility 0.040** 0.062*** 0.078***\n(2.09) (6.35) (8.02)\nPP&E 0.027* 0.016 0.02\n(1.95) (1.53) (1.69)\nYear FE Yes Yes Yes\nIndustry FE Yes Yes Yes\nEstimation OLS OLS OLS\nAdjusted R2 0.097 0.102 0.109\nN 133,830 133,830 133,830\nPanel C. Incremental Informativeness\nDep Var I(Increase=1)\n(1) (2) (3) (4) (5)\nGPT 0.181*** 0.170*** 0.179***\n(3.43) (2.67) (3.35)\nANN 0.150*** 0.053**\n(3.69) (2.44)\nLogistic 0.088*** 0.068**\n(2.99) (2.05)\nYear FE Yes Yes Yes Yes Yes\nIndustry FE Yes Yes Yes Yes Yes\nAdjusted R2 0.056 0.051 0.032 0.061 0.06\nN 133,830 133,830 133,830 133,830 133,830\nFinancial Statement Analysis with Large Language Models 50\nTable 5 .Experimental Variations and GPT’s Predictability\nWe compare the predictive performance of the model based on several experimental settings. Conf Score is the confidence score\n(ranging from 0 to 1) that the model produces. Confidence score measures how certain the model is in its answers. Log Prob is\nthe averaged token-level logistic probabilities. High and Low in columns (1), (2), (3), and (4) denote first and fourth quartiles,\nrespectively. Magnitude is the predicted magnitude in earnings change provided by the model. LLM Version denotes the family\nof LLM that we use for the experiment. Accuracy is the percentage of correct predictions out of total predictions. F1 is the\nharmonic mean of the precision and recall. In Panel B, we report the model performance of an ANN model based on text\nembedding. We use BERT-base-uncased model to extract contextualized embedding representation of the narrative financial\nstatement analysis performed by the model. The input layer as 768 dimensions, two hidden layers have 256 and 64 dimensions\neach, and the final layer has one dimension. We use ReLU activation function in the first two transitions and sigmoid for the last\ntransition. Batch size is 128. We use Adam optimizer and binary cross-entropy loss. The model is trained on rolling five-year\ntraining windows and hyper-parameters (learning rate and dropout) are determined based on a grid-search on the random 20%\nof the training sample. ANN with Financial Statement Variables denotes the model in Table 4, Panel A. AUC denotes the\narea-under-the-curve.\nConf Score Log Prob Magnitude LLM Version\nHigh Low High Low Large Small GPT4.0 GPT3.5 Gemini\n(1) (2) (3) (4) (5) (6) (7) (8) (9)\nAccuracy 62.44% 59.86% 63.15% 58.54% 62.03% 60.22% 61.05% 52.29% 59.15%\nF1 66.47% 55.62% 65.16% 54.15% 61.16% 57.95% 65.82% 59.17% 62.23%\nFinancial Statement Analysis with Large Language Models 51\nTable 6 .Memory of GPT\nIn this table, we test GPT’s memory. For Panel A and Panel B, we ask GPT to provide ten most probable names of the\ncompany and the most probable fiscal year, based on the standardized and anonymous financial statement information. In\nPanel A, we do not provide chain-of-thought prompts and in Panel B, we provide the same chain-of-thought prompts as in the\nmain analyses. In Panel C, we repeat the main analyses for fiscal year 2022 to predict 2023 earnings. GPT’s training window\nterminates in April 2023 and our sample period provides a perfect out-of-sample test. Accuracy is the percentage of correct\npredictions out of total predictions. F1 is the harmonic mean of the precision and recall\nPanel A. Without Chain-of-Thought\nAccuracy F1 Score\nFirm Name 0.07% 0.07%\nYear 2.95% 0.41%\nPanel B. Chain-of-Thought\nAccuracy F1 Score\nFirm Name 0.09% 0.09%\nYear 1.01% 0.27%\nPanel C. Out-of-Sample (Using 2022 data to predict fiscal year 2023)\nAccuracy F1 Score\nLogistic Regression 54.47% 60.60%\nANN 59.10% 61.13%\nGPT 58.96% 63.91%\nAnalyst 53.06% 58.95%\nFinancial Statement Analysis with Large Language Models 52\nTable 7 .Predictive Ability of GPT-Generated Texts\nWe report the model performance of an ANN model based on text embedding. We use BERT-base-uncased model to extract\ncontextualized embedding representation of the narrative financial statement analysis performed by the model. The input layer\nas 768 dimensions, two hidden layers have 256 and 64 dimensions each, and the final layer has two dimensions (probability\nvector). We use ReLU activation function in the first two transitions and sigmoid for the last transition. Batch size is 128. We\nuse Adam optimizer and cross-entropy loss. The model is trained on rolling five-year training windows and hyper-parameters\n(learning rate and dropout) are determined based on a grid-search on the random 20% of the training sample. ANN with\nFinancial Statement Variables denotes the model in Table 4, Panel A. ANN with Text and FS variables denotes a model that\nallows full non-linear interactions among embedding neurons and FS variables. ANN with Adjusted Text Embedding denotes\nmodels with adjusted text inputs. GPT produces three main textual outputs - trend, ratio, and rationale. ANN excl. Trend\ndenotes an ANN with an input embedding with only ratio and rationale analyses. ANN excl. Ratio and ANN excl. Rationale\nare defined likewise. AUC denotes the area-under-the-curve.\nAccuracy F1 Score AUC\nANN with GPT Text Embedding 58.95% 65.26% 64.22%\nANN with Financial Statement Variables 60.12% 61.30% 59.13%\nANN with Text and FS Variables 63.16% 66.33% 65.90%\nANN with Adjusted Text Embedding\nANN excl. Trend 57.11% 64.03% 63.81%\nANN excl. Ratio 55.65% 62.36% 61.89%\nANN excl. Rationale 58.88% 65.15% 64.16%\nFinancial Statement Analysis with Large Language Models 53\nTable 8 .Asset Pricing Implications\n*, **, and *** denote statistical significance at 10%, 5%, and 1% levels, respectively.\nIn this table, we show asset pricing implications of GPT’s predictions. We form portfolios on June 30 of each year and hold\nthe portfolios for one year. To form portfolios based on GPT’s predictions, for each fiscal year, we choose stocks with a binary\nprediction of “increase” and a magnitude prediction of either “moderate” or ”large. ” Then we sort those stocks on descending\naverage log probability values. From this selected subset of stocks, we long stocks equivalent to 10% of the entire stocks\navailable in the given fiscal year from those ranked highest in log probability. We also do the same for the stocks with a binary\nprediction of “decrease.” We filter stocks with a predicted magnitude change of either “moderate” or ”large”, and sort them on\nlog probability values. For ANN and logit, we sort the stocks on the predicted probability values of earnings increase. Then on\nJune 30, we long stocks in the top decile and short stocks in the bottom decile. Panel A reports monthly Sharpe ratio. Panel\nB reports alphas based on CAPM, three-factor, four-factor, five-factor, and six-factor (five factors plus momentum).\nPanel A. Sharpe Ratios (monthly)\nEqual-Weighted Value-Weighted\n(1) (2) (3) (4) (5) (6)\nHigh Low H-L High Low H-L\nGPT Predictions\nRet 1.72 0.44 1.28 1.04 0.48 0.56\nStd 0.59 0.68 0.38 0.52 0.69 0.38\nSharpe 2.92 0.65 3.36 2.00 0.70 1.47\nANN\nRet 1.40 0.51 0.89 1.11 0.59 0.52\nStd 0.72 0.67 0.35 0.61 0.88 0.29\nSharpe 1.94 0.76 2.54 1.82 0.67 1.79\nLogit\nRet 1.38 0.50 0.88 1.04 0.62 0.42\nStd 0.61 0.65 0.43 0.55 0.77 0.52\nSharpe 2.26 0.77 2.05 1.89 0.81 0.81\nPanel B. Alphas (monthly)\nEqual-Weighted Value-Weighted\nCAPM 3 Factor 4 Factor 5 Factor 5F+Mom CAPM 3 Factor 4 Factor 5 Factor 5F+Mom\n(1) (2) (3) (4) (5) (6) (7) (8) (9) (10)\nGPT\nHigh 1.03*** 1.04*** 1.05*** 1.02*** 1.03*** 0.48*** 0.58*** 0.61*** 0.52*** 0.55***\n(10.47) (16.28) (16.21) (15.67) (14.78) (5.93) (7.82) (8.08) (6.78) (7.35)\nLow -0.20 -0.29** -0.24** 0.05 0.19* -0.23 -0.42*** -0.28* -0.04 0.18\n(-1.24) (-2.46) (-2.03) (0.40) (1.65) (-1.45) (-2.91) (-1.96) (-0.27) (1.34)\nH – L 1.23*** 1.33*** 1.29*** 0.97*** 0.84*** 0.71*** 1.00*** 0.89*** 0.56*** 0.37**\n(8.96) (10.48) (10.10) (3.14) (4.48) (3.81) (6.27) (5.57) (3.78) (2.43)\nANN\nHigh 0.98*** 1.00*** 0.99*** 0.85*** 0.82*** 0.55*** 0.59*** 0.65*** 0.56*** 0.52***\n(8.35) (14.23) (11.32) (10.06) (8.55) (6.30) (8.16) (9.34) (6.95) (6.88)\nLow -0.13 -0.23*** 0.02 0.16 0.22 -0.35* -0.49*** -0.23 -0.16 0.02\n(-1.16) (-2.99) (0.32) (0.62) (1.53) (-1.73) (-3.19) (-1.16) (-0.89) (0.66)\nH – L 1.11*** 1.23*** 0.97*** 0.69** 0.60* 0.90*** 1.08*** 0.88*** 0.72*** 0.50***\n(7.62) (11.32) (9.38) (2.15) (1.89) (4.23) (7.99) (6.00) (4.56) (3.19)\nLogit\nHigh 0.89*** 0.90*** 0.86*** 0.71*** 0.68*** 0.40*** 0.44*** 0.46*** 0.36*** 0.33**\n(7.15) (9.11) (9.25) (6.11) (4.23) (4.15) (4.22) (4.44) (2.86) (2.15)\nLow -0.18 -0.26* -0.05 0.23 0.25 -0.24 -0.36** -0.29* -0.11 0.02\n(-1.23) (-1.95) (-0.26) (1.06) (1.10) (-1.55) (-2.26) (-1.77) (-0.95) (0.05)\nH – L 1.07*** 1.16*** 0.91*** 0.48* 0.43* 0.64** 0.80** 0.75** 0.47 0.31\n(6.50) (8.15) (7.19) (2.06) (1.96) (2.35) (2.56) (2.41) (1.67) (1.55)\n",
    "constraint": [
      [
        "Company",
        "Executive",
        "Merger",
        "Transaction"
      ],
      [
        "reports",
        "discloses",
        "increases",
        "decreases",
        "forecasts",
        "impacts"
      ],
      [
        "FinancialMetric",
        "FiscalPeriod",
        "MarketSegment",
        "Regulation",
        "CashFlow",
        "Revenue",
        "NetIncome"
      ]
    ],
    "trajectory": {
      "get_default_schema": "The final extraction result should be formatted as a JSON object.",
      "extract_information_direct": [
        "Here's the extracted information in JSON format:\n\n```json\n{\n  \"Company\": \"University of Chicago, Booth School of Business\",\n  \"Executive\": [\n    \"Alex G. Kim\",\n    \"Maximilian Muhn\",\n    \"Valeri V. Nikolaev\"\n  ],\n  \"Merger\": null,\n  \"Transaction\": null,\n  \"reports\": null,\n  \"discloses\": null,\n  \"increases\": null,\n  \"decreases\": null,\n  \"forecasts\": null,\n  \"impacts\": null,\n  \"FinancialMetric\": [\n    \"Earnings\",\n    \"Revenue\",\n    \"NetIncome\",\n    \"CashFlow\"\n  ],\n  \"FiscalPeriod\": null,\n  \"MarketSegment\": null,\n  \"Regulation\": null,\n  \"Direction of earnings changes\": [\n    \"predict earnings changes\",\n    \"outperforms financial analysts\",\n    \"prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model\"\n  ],\n  \"Address\": \"5757 S. University Ave. Chicago, IL 60637\",\n  \"Phone\": \"773.702.5599\",\n  \"Website\": \"bfi.uchicago.edu\",\n  \"Companion App\": \"To showcase the capabilities of LLMs for financial statement analysis, we created an interactive Companion App.\",\n  \"Keywords\": [\n    \"Financial statement analysis\",\n    \"Large language models\",\n    \"GPT4\",\n    \"chain-of-thought\",\n    \"neural network\",\n    \"asset pricing\",\n    \"earnings\",\n    \"direction of earnings changes\",\n    \"analysts\"\n  ],\n  \"JEL Codes\": [\n    \"G12\",\n    \"G14\",\n    \"G41\",\n    \"M41\"\n  ],\n  \"Research Design\": [\n    \"Passing a balance sheet and income statement in a standardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze them.\",\n    \"Based on the analysis of the two financial statements, the model must decide whether a firm's economic performance is sustainable and, more specifically, whether a company's earnings will grow or decline in the following period.\"\n  ],\n  \"Research Questions\": [\n    \"Can an LLM successfully perform financial statement analysis in a way similar to what professional human analysts do?\"\n  ],\n  \"Authors\": [\n    \"Alex G. Kim\",\n    \"Maximilian Muhn\",\n    \"Valeri V",
        {
          "can_LLM_generate_economic_insights": true,
          "LLM_performance_compared_to_human_analysts": "LLM's performance is at a disadvantage compared to human analysts due to the complexity and ambiguity of financial statement analysis",
          "advantages_of_LLM": "vast knowledge and general understanding of the world, such as business concepts and investment theories that allow it to emulate deductive reasoning performed by humans",
          "approach_to_testing_LLM_performance": {
            "step1": "anonymize and standardize corporate financial statements to prevent the potential memory of the company by the language model",
            "step2": "design prompts that instruct the model to perform financial statement analysis and, subsequently, to determine the direction of future earnings"
          },
          "prompt_types": [
            "simple prompt",
            "Chain-of-Thought (CoT) prompt"
          ],
          "CoT_prompt": "implements the thought process of a financial analyst by identifying notable trends in financial statement line items, computing key financial ratios, synthesizing this information, and forming expectations about future earnings",
          "dataset": {
            "full_sample": {
              "period": "1968-2021",
              "observations": 150678,
              "firms": 15401
            },
            "analyst_sample": {
              "period": "1983-2021",
              "observations": 39533,
              "firms": 3152
            }
          },
          "target_variable": "directional change in future earnings",
          "analyst_prediction_accuracy": {
            "first-month": 53,
            "three-month": 56,
            "six-month": 57
          },
          "GPT_performance": {
            "simple_prompt": 52,
            "CoT_prompt": 60
          },
          "F1-score": {}
        },
        {
          "GPT_Performance": {
            "accuracy": 60.31,
            "F1_score": 63.45
          },
          "ANN_Performance": {
            "accuracy": 60.45,
            "F1_score": 61.62
          },
          "GPT_Comparison": {
            "GPT_Accuracy": 60.31,
            "ANN_Accuracy": 60.45,
            "GPT_F1_Score": 63.45,
            "ANN_F1_Score": 61.62
          },
          "ANN_Training_Data": {
            "data_source": "Compustat",
            "years": 5,
            "train_type": "out of sample"
          },
          "GPT_Hypotheses": {
            "hypothesis_1": "GPT's performance is driven by its (possibly near-perfect) memory",
            "hypothesis_2": "GPT generates useful insights based on which the model infers the direction of future earnings"
          },
          "ANN_Model_Type": {
            "model_type": "artificial neural network",
            "predictors": 59,
            "non_linearities": true,
            "interactions": true
          },
          "GPT_Model_Type": {
            "model_type": "large language model",
            "confidence": {
              "higher_confidence": "more accurate forecasts",
              "lower_confidence": "less accurate forecasts"
            }
          },
          "Other_LLMs": {
            "Gemini_Pro": {}
          },
          "Experiments": {
            "partitioning_samples": true,
            "confidence_in_answers": true,
            "different_families_of_LLMs": true
          },
          "Results_Generalization": {
            "GPT3.5": {
              "performance": "considerably less impressive"
            },
            "GPT4": {
              "performance": "impressive"
            },
            "Gemini_Pro": {
              "performance": "similar to GPT4"
            }
          }
        },
        "{\n  \"accuracy\": 59,\n  \"forecast_accuracy\": 60,\n  \"f1_score\": 65,\n  \"correlation\": 94,\n  \"ratio_analysis_importance\": \"most important\",\n  \"economic_usefulness\": {\n    \"alpha\": 12,\n    \"Sharpe_ratio\": null\n  },\n  \"company_size\": \"small\",\n  \"performance\": \"outperforms\",\n  \"literature_contributions\": [\n    \"large-scale evidence on LLM's ability to analyze financial statements\",\n    \"show that an LLM can generate state-of-the-art inferences about the direction of the company\",\n    \"provide evidence on the limits of LLMs\",\n    \"contribute to the literature on fundamental analysis\"\n  ],\n  \"financial_statement_analysis\": {\n    \"importance\": \"critical\",\n    \"tasks\": [\n      \"predicting firms\" earnings\",\n      \"establishing a baseline understanding of a company's financial position and performance\"\n    ]\n  },\n  \"analyst_performance\": {\n    \"accuracy\": null,\n    \"bias\": \"potentially erroneous or biased\"\n  },\n  \"general-purpose_language_models\": {\n    \"promise\": \"facilitating financial statement analysis and the associated tasks\"\n  }\n}",
        {
          "company": "null",
          "executive": "null",
          "merger": "null",
          "transaction": "null",
          "reports": "null",
          "discloses": "null",
          "increases": "null",
          "decreases": "null",
          "forecasts": "null",
          "impacts": "null",
          "financialMetric": "null",
          "fiscalPeriod": "null",
          "marketSegment": "null",
          "regulation": "null",
          "cashFlow": "null",
          "revenue": "null",
          "netIncome": "null"
        },
        "{\n  \"prompt_type\": \"Chain-of-Thought\",\n  \"instructions\": [\n    \"take on the role of a financial analyst\",\n    \"perform financial statement analysis\",\n    \"identify notable changes in certain financial statement items\",\n    \"compute key financial ratios\",\n    \"state the formulae first, and then perform simple computations\",\n    \"provide economic interpretations of the computed ratios\",\n    \"predict whether earnings are likely to increase or decrease in the subsequent period\",\n    \"produce a paragraph that elaborates its rationale\",\n    \"provide the predicted magnitude of earnings change and the confidence in its answer\"\n  ],\n  \"model\": \"gpt-4-0125-preview\",\n  \"temperature\": 0,\n  \"maxtokens\": null,\n  \"top_p_sampling\": 1,\n  \"logprobs\": true,\n  \"data_source\": \"Compustat\",\n  \"data_period\": \"1968-2021\",\n  \"test_data\": \"2022-2023\",\n  \"training_window\": \"April 2023\",\n  \"filtered_data\": 150,678,\n  \"distinct_firms\": 15,401,\n  \"balance_sheet\": true,\n  \"income_statement\": true,\n  \"analyst_sample\": 39,533,\n  \"analyst_forecasts\": true,\n  \"IBES_data\": true,\n  \"forecast_timings\": [\"1m\", \"3m\", \"6m\"],\n  \"GPT_prediction\": 53.0,\n  \"analyst_forecast_pessimism\": true,\n  \"company_characteristics\": {\n    \"Size\": null,\n    \"BtoM\": null,\n    \"Leverage\": null,\n    \"Earn Vol\": null\n  }\n}",
        {
          "accuracy": 52.33,
          "F1_score": 54.52,
          "model": "GPT",
          "prompt": "analyze financial statements and predict the direction of future earnings",
          "CoT_based_performance": {
            "accuracy": 60.35,
            "difference": 7,
            "statistical_significance": "1%"
          },
          "analyst_performance": {
            "accuracy": 52.71,
            "F1_score": 54.48
          },
          "evaluation_metrics": {
            "accuracy": "percentage of correctly predicted cases scaled by the total number of predictions made",
            "F1_score": "harmonic mean of precision and recall"
          },
          "prediction_methods": {
            "naive_model": {
              "accuracy": 49.11,
              "F1_score": 53.02
            },
            "analysts_forecasts": {
              "accuracy": 56.58,
              "month": "six"
            }
          },
          "firm_characteristics": {},
          "OLS_regression": {}
        },
        "{\n  \"variables_winsorized\": \"1%\",\n  \"standard_errors_clustered\": \"SIC two-digit industry level\",\n  \"results_presented\": \"Table 3, Panel A, and Figure 2\",\n  \"column_1_results\": {\n    \"firm_size\": \"smaller in size\",\n    \"leverage_ratio\": \"higher leverage ratio\",\n    \"loss\": \"records a loss\",\n    \"earnings_volatility\": \"exhibits volatile earnings\"\n  },\n  \"column_2_3_4_results\": {\n    \"firm_size\": \"smaller in size\",\n    \"magnitude\": \"nearly half compared to column (1)\",\n    \"p_value\": \"< 1%\",\n    \"analyst_difficulty\": \"analysts face difficulties in predicting small firms\" earnings\"\n  },\n  \"analyst_forecast_measures\": \"all analyst forecast measures\",\n  \"column_5_6_7_results\": {\n    \"GPT_coefficient\": \"statistically significant\",\n    \"analyst_coefficient\": \"statistically significant\",\n    \"adjusted_R_squared\": \"0.089\"\n  },\n  \"ex-ante_bias\": {\n    \"forecast_errors\": \"difference between actual EPS and forecasted EPS, scaled by the stock price at the end of the last fiscal year\",\n    \"absolute_value\": \"absolute value of the fitted values from the regression\"\n  },\n  \"disagreement_in_analysts_forecasts\": {\n    \"standard_deviation\": \"standard deviation of analysts\" forecasted EPS values, scaled by the stock price at the end of the preceding fiscal year\"\n  },\n  \"sample_partitioned\": \"based on the quartile values of analyst bias\"\n}",
        {
          "GPT_Value": {
            "description": "GPT is more valuable in situations when human analysts are likely to be biased.",
            "accuracy": 60.31,
            "F1_score": 63.45
          },
          "ANN_Model": {
            "accuracy": 60.45,
            "F1_score": 61.62
          },
          "Input_Variables": {
            "GPT": "balance sheet and income statement variables",
            "ANN": "59 Ou and Penman (1989) ratios"
          },
          "ANN_Model_Financial_Statement": {
            "accuracy": 60.12,
            "F1_score": 61.3
          },
          "Time_Trends": {
            "GPT": {
              "accuracy_decrease": 0.1,
              "total_decrease": 5.4
            },
            "ANN": {},
            "Sharp_Drops": [
              "1974",
              "2008-2009",
              "2020"
            ]
          }
        },
        {
          "evaluation_metrics": "negative and statistically significant time trend",
          "GPT_performance": "outperforms stepwise logistic predictions at 1% level",
          "GPTvsANN": "not statistically significant at conventional levels",
          "factors_associated_with_inaccuracy": [
            "smaller firms",
            "higher leverage",
            "record a loss",
            "higher earnings volatility"
          ],
          "ANNvsGPT_inaccuracy": {
            "firm_size": "5.5 percentage point decrease in prediction accuracy",
            "loss": "statistically different at the 5% level",
            "earnings_volatility": "statistically different at the 5% level"
          },
          "GPT_capabilities": "rely on conceptual knowledge and theoretical understanding of business",
          "incremental_informativeness": {
            "predicted_earnings_changes": "positively associated with actual changes",
            "GPTvsANN": "GPT captures some additional dimensions of information"
          },
          "confidence": {
            "method": "two methods: reported confidence score and token-level logistic probability values",
            "results": {
              "high_confidence": "62.44% (63.15%) average accuracy",
              "low_confidence": "approximately 2.6 (4.6) percentage points lower than high confidence group"
            }
          },
          "magnitude": {
            "method": "instruct the model to provide expected magnitude of earnings change",
            "results": {
              "large_changes": "62.03% average accuracy",
              "small_changes": "60.22% average accuracy"
            }
          },
          "LLM_type": {
            "method": "test whether the capabilities associated with a specific LLM type determine its predictive ability",
            "results": {
              "GPT-4-turbo": "achieves the best performance",
              "Gemini Pro 1.5": "achieves an overall accuracy of 59.15%",
              "GPT-3.5-turbo": "achieves an accuracy of only 52.29%"
            }
          }
        },
        {
          "Section": "Where Does an LLM’s Predictive Ability Come From?",
          "Explanation1": "GPT’s performance comes from its memory, e.g., due to the model’s ability to identify the company based on numeric data.",
          "Explanation2": "The strength of the model is in its ability to generate narrative insights based on its analysis of numeric data.",
          "Concern": "Look-ahead bias",
          "BiasDescription": "The model may have been trained on the company-specific financial data and, hence, already may “know” the answer as to whether earnings increased or decreased in the future.",
          "Mitigation": {
            "AnonymizedFormat": true,
            "NoDates": true,
            "RelativeYears": true
          },
          "Test1": {
            "Description": "Can GPT Guess Firm Name and Year?",
            "Accuracy": {
              "FirmName": 0.07,
              "Year": 2.95
            },
            "Results": {
              "FirmName": "Tesla, Facebook, and Amazon",
              "Year": "2019, 2020, or 2021"
            }
          },
          "Test2": {
            "Description": "Chain-of-thought prompts",
            "Accuracy": {
              "FirmName": "low accuracy",
              "Year": "low accuracy"
            }
          },
          "AnalysisOutsideTrainingWindow": {
            "Description": "Analysis outside of GPT’s training window",
            "Accuracy": 58.96,
            "F1Score": 63.91
          },
          "OutputSchema": {
            "Properties": [
              "Company",
              "Executive",
              "Merger",
              "Transaction",
              "reports",
              "discloses",
              "increases",
              "decreases",
              "forecasts",
              "impacts",
              "FinancialMetric",
              "FiscalPeriod",
              "MarketSegment",
              "Regulation",
              "CashFlow",
              "Revenue",
              "NetIncome"
            ]
          }
        },
        {
          "GPT_Out_of_Sample_Performance": {
            "accuracy": 58.96,
            "F1_score": 63.91
          },
          "ANN_Model_Performance": {
            "accuracy": 59.1,
            "F1_score": 61.13
          },
          "Commonly_Used_Bigrams": [
            "operating margin",
            "efficiency (asset and inventory turnover)",
            "liquidity (current ratio, current assets, and current liability)"
          ],
          "Model_Rationale": [
            "firm growth",
            "liquidity",
            "operating profitability",
            "efficiency"
          ],
          "Embeddings_Model_Performance": {
            "accuracy": 58.95,
            "F1_score": 65.26,
            "AUC": 64.22
          },
          "Correlation_Between_GPT_Forecasts_and_Embeddings_Model": 94,
          "Dual_Input_Model_Performance": {
            "accuracy": 63.16,
            "F1_score": 66.33,
            "AUC": 65.9
          }
        },
        "{\n  \"GPT_Model_Accuracy\": 57.11,\n  \"ANN_Model_Accuracy_Excluding_Trend_Analysis\": 57.11 - 1.8,\n  \"ANN_Model_Accuracy_Excluding_Ratio_Analysis\": 55.65,\n  \"ANN_Model_Accuracy_Excluding_Rationale\": 58.88,\n  \"Ratio_Analysis_Importance\": \"highest\",\n  \"Trend_Analysis_Importance\": \"second highest\",\n  \"Rationale_Analysis_Importance\": null,\n  \"Asset_Pricing_Models_Proxy\": \"current level of profitability\",\n  \"GPT_Forecasts_Incremental_Information\": true,\n  \"Sharpe_Ratio_GPT_Equal_Weighted\": 3.36,\n  \"Sharpe_Ratio_GPT_Value_Weighted\": 1.47,\n  \"Sharpe_Ratio_Ann_Equal_Weighted\": 2.54,\n  \"Sharpe_Ratio_Ann_Value_Weighted\": 1.79,\n  \"Sharpe_Ratio_LogisticRegression_Equal_Weighted\": 2.05,\n  \"Sharpe_Ratio_LogisticRegression_Value_Weighted\": 0.81,\n  \"Monthly_Alphas_GPT\": 1.29,\n  \"Monthly_Alphas_Ann\": null,\n  \"Monthly_Alphas_LogisticRegression\": null,\n  \"Factor_Models\": [\"CAPM\", \"Fama and French (2015)\"s five factors plus momentum\"]\n}",
        {
          "GPT Alpha": 84,
          "GPT Annual Alpha": 10,
          "ANN Alpha": 60,
          "ANN T-Statistic": 1.89,
          "Logistic Regression Alpha": 43,
          "Logistic Regression T-Statistic": 1.96,
          "GPT Sharpe Ratio (Equal-Weighted)": 2.84,
          "GPT Sharpe Ratio (Value-Weighted)": 0.95,
          "ANN Alpha (Value-Weighted)": 50,
          "GPT Alpha (Value-Weighted)": 37,
          "Logit Alpha": 31,
          "Logit T-Statistic": 1.55,
          "Model Performance Advantage": "GPT's performance is on par (or even better in some cases) with that of the most sophisticated narrowly specialized machine learning models, namely, an ANN trained on earnings prediction tasks."
        },
        {
          "FinancialMetric": [
            "Earnings",
            "Revenue",
            "NetIncome",
            "CashFlow"
          ],
          "FiscalPeriod": [
            "Annual",
            "Quarterly"
          ],
          "Regulation": [
            "Sarbanes-Oxley Act"
          ]
        },
        {
          "FiscalYear": 1980,
          "Value": 61.98
        },
        {
          "mean": 0.53,
          "std": 0.499,
          "p25": 0,
          "p50": 1,
          "p75": 1
        },
        {
          "mean": 0.58,
          "std": 0.256,
          "p25": 0.4,
          "p50": 0.582,
          "p75": 0.771
        },
        {
          "Accuracy": {
            "FirmName": 0.07,
            "Year": 2.95
          },
          "F1Score": {
            "FirmName": 0.07,
            "Year": 0.41
          }
        },
        {
          "capm": 1.03,
          "threeFactor": 1.04,
          "fourFactor": 1.05,
          "fiveFactor": 1.02,
          "fiveFmom": 1.03,
          "low": -0.2,
          "highLow": 1.23
        }
      ],
      "summarize_answer": "Here's a summary of the information extracted from the result list:\n\n**Company Information**\n\n* Company Name: University of Chicago, Booth School of Business\n* Address: 5757 S. University Ave. Chicago, IL 60637\n* Phone: 773.702.5599\n* Website: bfi.uchicago.edu\n\n**Executive Team**\n\n* Executive Members:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**Financial Metrics**\n\n* Financial Metrics:\n  + Earnings\n  + Revenue\n  + Net Income\n  + Cash Flow\n\n**Research Design**\n\n* Research Design:\n  + Passing a balance sheet and income statement in a standardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze them.\n  + Based on the analysis of the two financial statements, the model must decide whether a firm's economic performance is sustainable and, more specifically, whether a company's earnings will grow or decline in the following period.\n\n**Research Questions**\n\n* Research Questions:\n  + Can an LLM successfully perform financial statement analysis in a way similar to what professional human analysts do?\n\n**Authors**\n\n* Authors:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**LLM Performance**\n\n* LLM Performance:\n  + GPT's performance is at a disadvantage compared to human analysts due to the complexity and ambiguity of financial statement analysis.\n  + LLM's performance is on par with the performance of a narrowly trained state-of-the-art ML model.\n  + GPT's performance comes from its memory, e.g., due to the model's ability to identify the company based on numeric data.\n  + The strength of the model is in its ability to generate narrative insights based on its analysis of numeric data.\n\n**Model Comparison**\n\n* Model Comparison:\n  + GPT vs. ANN: GPT outperforms ANN in some cases, but ANN performs better in others.\n  + GPT vs. Logistic Regression: GPT outperforms Logistic Regression in some cases, but Logistic Regression performs better in others.\n\n**Output Schema**\n\n* Output Schema:\n  + Properties:\n    - Company\n    - Executive\n    - Merger\n    - Transaction\n    - Reports\n    - Discloses\n    - Increases\n    - Decreases\n    - Forecasts\n    - Impacts\n    - Financial Metric\n    - Fiscal Period\n    -"
    },
    "pred": "Here's a summary of the information extracted from the result list:\n\n**Company Information**\n\n* Company Name: University of Chicago, Booth School of Business\n* Address: 5757 S. University Ave. Chicago, IL 60637\n* Phone: 773.702.5599\n* Website: bfi.uchicago.edu\n\n**Executive Team**\n\n* Executive Members:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**Financial Metrics**\n\n* Financial Metrics:\n  + Earnings\n  + Revenue\n  + Net Income\n  + Cash Flow\n\n**Research Design**\n\n* Research Design:\n  + Passing a balance sheet and income statement in a standardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze them.\n  + Based on the analysis of the two financial statements, the model must decide whether a firm's economic performance is sustainable and, more specifically, whether a company's earnings will grow or decline in the following period.\n\n**Research Questions**\n\n* Research Questions:\n  + Can an LLM successfully perform financial statement analysis in a way similar to what professional human analysts do?\n\n**Authors**\n\n* Authors:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**LLM Performance**\n\n* LLM Performance:\n  + GPT's performance is at a disadvantage compared to human analysts due to the complexity and ambiguity of financial statement analysis.\n  + LLM's performance is on par with the performance of a narrowly trained state-of-the-art ML model.\n  + GPT's performance comes from its memory, e.g., due to the model's ability to identify the company based on numeric data.\n  + The strength of the model is in its ability to generate narrative insights based on its analysis of numeric data.\n\n**Model Comparison**\n\n* Model Comparison:\n  + GPT vs. ANN: GPT outperforms ANN in some cases, but ANN performs better in others.\n  + GPT vs. Logistic Regression: GPT outperforms Logistic Regression in some cases, but Logistic Regression performs better in others.\n\n**Output Schema**\n\n* Output Schema:\n  + Properties:\n    - Company\n    - Executive\n    - Merger\n    - Transaction\n    - Reports\n    - Discloses\n    - Increases\n    - Decreases\n    - Forecasts\n    - Impacts\n    - Financial Metric\n    - Fiscal Period\n    -"
  },
  "frontend_schema": "",
  "frontend_res": "Here's a summary of the information extracted from the result list:\n\n**Company Information**\n\n* Company Name: University of Chicago, Booth School of Business\n* Address: 5757 S. University Ave. Chicago, IL 60637\n* Phone: 773.702.5599\n* Website: bfi.uchicago.edu\n\n**Executive Team**\n\n* Executive Members:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**Financial Metrics**\n\n* Financial Metrics:\n  + Earnings\n  + Revenue\n  + Net Income\n  + Cash Flow\n\n**Research Design**\n\n* Research Design:\n  + Passing a balance sheet and income statement in a standardized form to the large language model, GPT 4.0 Turbo, and asking the model to analyze them.\n  + Based on the analysis of the two financial statements, the model must decide whether a firm's economic performance is sustainable and, more specifically, whether a company's earnings will grow or decline in the following period.\n\n**Research Questions**\n\n* Research Questions:\n  + Can an LLM successfully perform financial statement analysis in a way similar to what professional human analysts do?\n\n**Authors**\n\n* Authors:\n  + Alex G. Kim\n  + Maximilian Muhn\n  + Valeri V. Nikolaev\n\n**LLM Performance**\n\n* LLM Performance:\n  + GPT's performance is at a disadvantage compared to human analysts due to the complexity and ambiguity of financial statement analysis.\n  + LLM's performance is on par with the performance of a narrowly trained state-of-the-art ML model.\n  + GPT's performance comes from its memory, e.g., due to the model's ability to identify the company based on numeric data.\n  + The strength of the model is in its ability to generate narrative insights based on its analysis of numeric data.\n\n**Model Comparison**\n\n* Model Comparison:\n  + GPT vs. ANN: GPT outperforms ANN in some cases, but ANN performs better in others.\n  + GPT vs. Logistic Regression: GPT outperforms Logistic Regression in some cases, but Logistic Regression performs better in others.\n\n**Output Schema**\n\n* Output Schema:\n  + Properties:\n    - Company\n    - Executive\n    - Merger\n    - Transaction\n    - Reports\n    - Discloses\n    - Increases\n    - Decreases\n    - Forecasts\n    - Impacts\n    - Financial Metric\n    - Fiscal Period\n    -"
}